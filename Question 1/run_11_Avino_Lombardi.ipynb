{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4828f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Tuple, List\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "# Import functions from Functions.py file\n",
    "from Functions_11_Avino_Lombardi import (\n",
    "    forward, backward,\n",
    "    g1, dg1_dx, g2, dg2_dx,\n",
    "    mse_loss, mape,\n",
    "    initialize_parameters, unroll_params, roll_params,\n",
    "    check_gradients_with_central_differences\n",
    ")\n",
    "\n",
    "\n",
    "# --- Define Objective Function for scipy.optimize.minimize ---\n",
    "\n",
    "def objective_function(flat_params: np.ndarray,\n",
    "                       input_data: np.ndarray,\n",
    "                       target_data: np.ndarray,\n",
    "                       W_shapes: List[Tuple[int, ...]],\n",
    "                       b_shapes: List[Tuple[int, ...]],\n",
    "                       v_shape: Tuple[int, ...],\n",
    "                       activation_func: Callable[[np.ndarray], np.ndarray],\n",
    "                       activation_prime: Callable[[np.ndarray], np.ndarray],\n",
    "                       regularization_factor: float,\n",
    "                       num_layers: int) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Objective function for scipy.optimize.minimize.\n",
    "    Returns loss and flattened gradients.\n",
    "    \"\"\"\n",
    "    W_list, b_list, v = roll_params(flat_params, W_shapes, b_shapes, v_shape)\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred, a_list, z_list = forward(input_data, W_list, b_list, v, activation_func, num_layers)\n",
    "\n",
    "    # Calculate MSE loss (1/N * sum((y_i - y_hat_i)^2))\n",
    "    loss = mse_loss(target_data, y_pred)\n",
    "\n",
    "    # Add L2 regularization to the loss: lambda * sum( ||omega_l||_F^2 )\n",
    "    regularization_term = 0\n",
    "    for W in W_list:\n",
    "        regularization_term += np.sum(W**2)\n",
    "    regularization_term += np.sum(v**2) # Sum of squares for final output weights\n",
    "    loss += regularization_factor * regularization_term \n",
    "\n",
    "    # Backward pass to get gradients\n",
    "    grad_W_list, grad_b_list, grad_v = backward(input_data, target_data, W_list, b_list, v, a_list, z_list, activation_prime, num_layers)\n",
    "\n",
    "    # Add regularization term to gradients\n",
    "    for i in range(len(grad_W_list)):\n",
    "        grad_W_list[i] += 2 * regularization_factor * W_list[i] # d/dW(||W||^2) = 2W\n",
    "    grad_v += 2 * regularization_factor * v # d/dv(||v||^2) = 2v\n",
    "\n",
    "    # Flatten gradients for the optimizer\n",
    "    flattened_gradients = unroll_params(grad_W_list, grad_b_list, grad_v)\n",
    "\n",
    "    return loss, flattened_gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5365d39",
   "metadata": {},
   "source": [
    "### Data loading and splitting\n",
    "After loading the data in our environment, we randomly partition it into training ( $80\\% $) and testing ( $20\\%$ ) sets. This separation is fundamental to evaluate the generalization capability of our trained model on unseen (test) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af05b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape: (20475, 32)\n",
      "Initial y shape: (20475, 1)\n",
      "D_input (number of input features): 32\n",
      "y_output_dim (number of output dimensions): 1\n",
      "\n",
      "X_train shape after split: (32, 16380)\n",
      "y_train shape after split: (1, 16380)\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading ---\n",
    "\n",
    "df = pd.read_csv('/Users/Val/Documents/GitHub/OMDS-Project/dataset/AGE_PREDICTION.csv')\n",
    "\n",
    "# Separate features (X_full) and target (y_full) from the entire dataset\n",
    "feature_columns = [f'feat_{i}' for i in range(1, 33)]\n",
    "X_full = df[feature_columns].values # Shape will be [N, D]\n",
    "y_full = df['gt'].values.reshape(-1, 1) # Shape will be [N, 1]\n",
    "\n",
    "\n",
    "print(f\"Initial X shape: {X_full.shape}\")\n",
    "print(f\"Initial y shape: {y_full.shape}\")\n",
    "\n",
    "# Get total number of samples\n",
    "N_samples_full = X_full.shape[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=1234)\n",
    "\n",
    "X_train = X_train.T # Transpose to [D, N]\n",
    "y_train = y_train.T # Transpose to [1, N]\n",
    "X_test = X_test.T # Transpose to [D, N]\n",
    "y_test = y_test.T # Transpose to [1, N]\n",
    "\n",
    "# Determine D_input (number of input features) - from training data\n",
    "D_input = X_train.shape[0]\n",
    "y_output_dim = y_train.shape[0] # Should be 1 for your problem\n",
    "print(f\"D_input (number of input features): {D_input}\")\n",
    "print(f\"y_output_dim (number of output dimensions): {y_output_dim}\\n\")\n",
    "print(f\"X_train shape after split: {X_train.shape}\")\n",
    "print(f\"y_train shape after split: {y_train.shape}\") # This should be (1, N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d1cb2",
   "metadata": {},
   "source": [
    "### Feature standardization\n",
    "\n",
    "Before applying any optimization routine, we normalize our data\n",
    "For each feature $x_i$:\n",
    "$$x_i^{\\text{normalized}}=\\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "\n",
    "where $\\mu_i$ and $\\sigma_i$ are the **mean** and **standard deviation** of the $i^{th}$ feature, computed over the **training set**, since at this stage we do not have access to test information. The same transformation is then applied to test data.\n",
    "\n",
    "Standardization ensures __all features contribute equally__ to the loss landscape, thus to the gradient updates, prevents issues like vanishing or exploding gradients due to varying feature scales, and accelerates the convergence of our L-BFGS-B optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --FEATURES NORMALIZATION -- also with \"StandardScaler\"\n",
    "# only computed from train data\n",
    "mu = X_train.mean(axis=1, keepdims=True)\n",
    "sigma = X_train.std(axis=1, keepdims=True)\n",
    "\n",
    "# Handle cases where standard deviation might be zero (e.g.,  constant feature)\n",
    "sigma[sigma == 0] = 1e-8 \n",
    "\n",
    "# Apply the transformation to the TRAINING DATA\n",
    "X_train_normalized = (X_train - mu) / sigma\n",
    "\n",
    "# Apply the *SAME* transformation (using mu and sigma from training) to the TEST DATA\n",
    "X_test_normalized = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4344d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing gradient check...\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "\n",
      "--- Gradient Check (Central Differences) ---\n",
      "Checking 200 parameters...\n",
      "Analytical loss at initial point: 825.524543\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of y_pred_for_grad_calc: (1, 1000)\n",
      "DEBUG IN BACKWARD: Shape of dLoss_dout: (1, 1000)\n",
      "\n",
      "Gradient check PASSED! Norm of difference: 6.429132e-07\n",
      "------------------------------------------\n",
      "Gradient check finished.\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT CHECKING\n",
    "print(\"\\nPerforming gradient check...\")\n",
    "\n",
    "# toy subset of training data for gradient check \n",
    "num_samples_for_check = min(1000, X_train_normalized.shape[1])\n",
    "X_check_subset = X_train_normalized[:, :num_samples_for_check]\n",
    "y_check_subset = y_train[:, :num_samples_for_check]\n",
    "\n",
    "# representative set of hyperparameters for the check\n",
    "check_L = 3\n",
    "check_neurons_config = [5, 5]\n",
    "check_activation_func = g1 # Using g1 for this check, then switching\n",
    "check_activation_prime = dg1_dx\n",
    "check_reg_factor = 0.01\n",
    "\n",
    "# Initialize parameters for the check (ensure these are fresh initializations)\n",
    "W_check_init, b_check_init, v_check_init = initialize_parameters(\n",
    "    D_input, check_neurons_config, y_output_dim, check_reg_factor\n",
    ")\n",
    "initial_flat_params_for_check = unroll_params(W_check_init, b_check_init, v_check_init)\n",
    "\n",
    "W_shapes_for_check = [W.shape for W in W_check_init]\n",
    "b_shapes_for_check = [b.shape for b in b_check_init]\n",
    "v_shape_for_check = v_check_init.shape\n",
    "\n",
    "# Call the gradient check function\n",
    "check_gradients_with_central_differences(\n",
    "    initial_flat_params_for_check,\n",
    "    X_check_subset, y_check_subset,\n",
    "    W_shapes_for_check, b_shapes_for_check, v_shape_for_check,\n",
    "    check_activation_func, check_activation_prime,\n",
    "    check_reg_factor, check_L,\n",
    "    objective_function # Pass a reference to your objective_function\n",
    ")\n",
    "\n",
    "print(\"Gradient check finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f9b13",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "We now perform **5-fold cross-validation** on the training data to assess the optimal Multi-Layer Perceptron (MLP) network architecture. This method involves partitioning the training set into k=5 equally sized, disjoint subsets. For each fold $i\\in\\{1,,5\\}$, the model is trained on the data from the other 4 folds and subsequently evaluated on the held-out fold $F_i$\n",
    "\n",
    "This cross-validation procedure is integrated with a **Grid Search** strategy. The Grid Search exhaustively explores a predefined hyperparameter space **H**, which includes combinations of:\n",
    "\n",
    "- Number of layers ($L\\in\\{2,3,4\\}$)\n",
    "\n",
    "- Number of neurons per hidden layer (e.g., $[8]$ for L=2, $[8,8]$ for L=3, etc.)\n",
    "\n",
    "- Choice of activation function ( sigmoid or hyperbolic tangent)\n",
    "\n",
    "- Regularization factor ($\\lambda$) for the L2 penalty.\n",
    "\n",
    "For each unique combination of hyperparameters within this grid, a model is trained and evaluated $5$ times (once for each fold). The \"best performance on average on validation sets\" refers to the mean evaluation metric (specifically, the Mean Absolute Percentage Error, MAPE) computed across these $5$ validation folds for that particular hyperparameter combination. The **optimal MLP architecture** is then identified as the combination of hyperparameters from the grid that yields the **lowest average MAPE** across its respective cross-validation folds. This averaging process provides a more robust and reliable estimate of the model's performance by reducing the variance associated with a single train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5244d8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning...\n",
      "\n",
      "Testing L=2, Neurons=[4], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2758%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[4], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.4946%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[4], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 23.5948%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[4], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2387%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[4], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.4948%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[4], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 23.6110%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[8], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2650%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[8], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3238%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[8], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 21.6838%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[8], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2119%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[8], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3776%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[8], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 21.7187%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[16], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.3437%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[16], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.2500%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[16], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.8928%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[16], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2046%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[16], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3265%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[16], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.9282%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[32], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.3571%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[32], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.2433%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[32], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.5922%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[32], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2029%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[32], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3094%\n",
      "--------------------------------------------------\n",
      "Testing L=2, Neurons=[32], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.6028%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[4, 4], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.3099%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[4, 4], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.4234%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[4, 4], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 31.4849%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[4, 4], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2790%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[4, 4], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.4828%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[4, 4], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 23.6893%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[8, 8], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.4426%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[8, 8], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3346%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[8, 8], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 21.6847%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[8, 8], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2192%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[8, 8], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3353%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[8, 8], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 21.7322%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[16, 16], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.4504%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[16, 16], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3131%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[16, 16], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.8933%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[16, 16], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2593%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[16, 16], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3056%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[16, 16], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.9386%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[32, 32], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 22.5429%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[32, 32], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.2995%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[32, 32], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.5862%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[32, 32], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.3449%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[32, 32], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.2977%\n",
      "--------------------------------------------------\n",
      "Testing L=3, Neurons=[32, 32], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.6364%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[4, 4, 4], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 23.1908%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[4, 4, 4], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 26.4760%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[4, 4, 4], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 31.3920%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[4, 4, 4], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2586%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[4, 4, 4], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.4903%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[4, 4, 4], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 23.8486%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[8, 8, 8], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.3532%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[8, 8, 8], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3453%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[8, 8, 8], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 21.6756%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[8, 8, 8], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.2190%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[8, 8, 8], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3479%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[8, 8, 8], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 21.7918%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[16, 16, 16], Activation=g1, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.8592%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[16, 16, 16], Activation=g1, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.3419%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[16, 16, 16], Activation=g1, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.8739%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[16, 16, 16], Activation=g2, Reg Factor=0.001\n",
      "  Average Validation MAPE: 20.3979%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[16, 16, 16], Activation=g2, Reg Factor=0.01\n",
      "  Average Validation MAPE: 20.2878%\n",
      "--------------------------------------------------\n",
      "Testing L=4, Neurons=[16, 16, 16], Activation=g2, Reg Factor=0.1\n",
      "  Average Validation MAPE: 20.9783%\n",
      "--------------------------------------------------\n",
      "\n",
      "Hyperparameter Tuning Complete.\n",
      "\n",
      "--- Optimal Hyperparameters and Performance ---\n",
      "Optimal Number of Layers (L): 2\n",
      "Optimal Number of Neurons per Layer (N): [32]\n",
      "Optimal Activation Function: g2\n",
      "Optimal Regularization Factor (lambda): 0.001\n",
      "Other Hyperparameters (e.g., initialization strategy): Xavier/Glorot initialization (scaled random normal)\n",
      "Optimization Solver: L-BFGS-B (CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH)\n",
      "Average Number of Iterations for Optimization: 2788.20\n",
      "Average Optimization Time per Fold: 214.6486 seconds\n",
      "Average Training Error (MSE, without regularization) of Optimal Model: 46.0791\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter Search Space ---\n",
    "hyperparameter_grid = {\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'num_neurons_per_layer': {\n",
    "        2: [[4], [8], [16], [32]], # For L=2\n",
    "        3: [[4, 4], [8, 8], [16, 16], [32, 32]], # For L=3\n",
    "        4: [[4, 4, 4], [8, 8, 8], [16, 16, 16]], # For L=4\n",
    "    },\n",
    "    'activation_function': [(g1, dg1_dx), (g2, dg2_dx)],\n",
    "    'regularization_factor': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001], # This was in a previous version, but might not be if you removed it for minimize.\n",
    "    'num_iterations': [500, 1000, 2000] # Same as above\n",
    "}\n",
    "\n",
    "# --- K-Fold Cross-Validation Setup ---\n",
    "n_splits = 5 # For 5-fold cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "\n",
    "best_mape = float('inf')\n",
    "best_hyperparameters = {}\n",
    "best_training_results = {}\n",
    "\n",
    "# --- Hyperparameter Tuning Loop ---\n",
    "print(\"Starting Hyperparameter Tuning...\\n\")\n",
    "\n",
    "for L in hyperparameter_grid['num_layers']:\n",
    "    hidden_neurons_options = hyperparameter_grid['num_neurons_per_layer'][L]\n",
    "\n",
    "    for neurons_config in hidden_neurons_options:\n",
    "        for activation_tuple in hyperparameter_grid['activation_function']:\n",
    "            activation_func, activation_prime = activation_tuple\n",
    "            activation_name = activation_func.__name__ # For printing\n",
    "\n",
    "            for reg_factor in hyperparameter_grid['regularization_factor']:\n",
    "\n",
    "                print(f\"Testing L={L}, Neurons={neurons_config}, Activation={activation_name}, Reg Factor={reg_factor}\")\n",
    "\n",
    "                fold_mape_scores = []\n",
    "                fold_training_errors = []\n",
    "                fold_optimization_times = []\n",
    "                fold_iterations = []\n",
    "\n",
    "                # Perform K-Fold Cross-Validation on the TRAINING DATA\n",
    "                for fold, (train_index, val_index) in enumerate(kf.split(X_train.T)): # X_train.T to split along samples\n",
    "                    X_fold_train, X_fold_val = X_train_normalized[:, train_index], X_train_normalized[:, val_index]\n",
    "                    y_fold_train, y_fold_val = y_train[:, train_index], y_train[:, val_index]\n",
    "\n",
    "                    # Initialize parameters for this fold\n",
    "                    W_init, b_init, v_init = initialize_parameters(D_input, neurons_config, y_output_dim, reg_factor)\n",
    "                    initial_flat_params = unroll_params(W_init, b_init, v_init)\n",
    "\n",
    "                    # Store shapes for reshaping inside objective function\n",
    "                    W_shapes = [W.shape for W in W_init]\n",
    "                    b_shapes = [b.shape for b in b_init]\n",
    "                    v_shape = v_init.shape\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    # Optimization using scipy.optimize.minimize on fold training data\n",
    "                    result = minimize(\n",
    "                        fun=objective_function,\n",
    "                        x0=initial_flat_params,\n",
    "                        args=(X_fold_train, y_fold_train, W_shapes, b_shapes, v_shape, activation_func, activation_prime, reg_factor, L),\n",
    "                        method='L-BFGS-B',\n",
    "                        jac=True,\n",
    "                        options={'disp': False, 'maxiter': 5000}\n",
    "                    )\n",
    "                    end_time = time.time()\n",
    "\n",
    "                    optimization_time = end_time - start_time\n",
    "                    fold_optimization_times.append(optimization_time)\n",
    "                    fold_iterations.append(result.nit)\n",
    "\n",
    "                    # Reshape optimized parameters\n",
    "                    W_optimized, b_optimized, v_optimized = roll_params(result.x, W_shapes, b_shapes, v_shape)\n",
    "\n",
    "                    # Evaluate on training set (without regularization for training error as per instructions)\n",
    "                    y_fold_train_pred_no_reg, _, _ = forward(X_fold_train, W_optimized, b_optimized, v_optimized, activation_func, L)\n",
    "                    train_error_no_reg = mse_loss(y_fold_train, y_fold_train_pred_no_reg)\n",
    "                    fold_training_errors.append(train_error_no_reg)\n",
    "\n",
    "                    # Evaluate on validation set using MAPE\n",
    "                    y_fold_val_pred, _, _ = forward(X_fold_val, W_optimized, b_optimized, v_optimized, activation_func, L)\n",
    "                    val_mape = mape(y_fold_val, y_fold_val_pred)\n",
    "                    fold_mape_scores.append(val_mape)\n",
    "\n",
    "                avg_mape = np.mean(fold_mape_scores)\n",
    "                avg_training_error = np.mean(fold_training_errors)\n",
    "                avg_optimization_time = np.mean(fold_optimization_times)\n",
    "                avg_iterations = np.mean(fold_iterations)\n",
    "\n",
    "                print(f\"  Average Validation MAPE: {avg_mape:.4f}%\")\n",
    "\n",
    "                if avg_mape < best_mape:\n",
    "                    best_mape = avg_mape\n",
    "                    best_hyperparameters = {\n",
    "                        'num_layers': L,\n",
    "                        'num_neurons_per_layer': neurons_config,\n",
    "                        'activation_function': activation_name,\n",
    "                        'regularization_factor': reg_factor\n",
    "                    }\n",
    "                    best_training_results = {\n",
    "                        'optimization_solver': result.message,\n",
    "                        'num_iterations': avg_iterations,\n",
    "                        'optimization_time': avg_optimization_time,\n",
    "                        'training_error_no_reg': avg_training_error,\n",
    "                        'best_model_params': result.x # Store the flat optimized parameters\n",
    "                    }\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Complete.\")\n",
    "\n",
    "# --- Final Evaluation and Output ---\n",
    "\n",
    "print(\"\\n--- Optimal Hyperparameters and Performance ---\")\n",
    "print(f\"Optimal Number of Layers (L): {best_hyperparameters['num_layers']}\")\n",
    "print(f\"Optimal Number of Neurons per Layer (N): {best_hyperparameters['num_neurons_per_layer']}\")\n",
    "print(f\"Optimal Activation Function: {best_hyperparameters['activation_function']}\")\n",
    "print(f\"Optimal Regularization Factor (lambda): {best_hyperparameters['regularization_factor']}\")\n",
    "print(f\"Other Hyperparameters (e.g., initialization strategy): Xavier/Glorot initialization (scaled random normal)\")\n",
    "print(f\"Optimization Solver: L-BFGS-B ({best_training_results['optimization_solver']})\")\n",
    "print(f\"Average Number of Iterations for Optimization: {best_training_results['num_iterations']:.2f}\")\n",
    "print(f\"Average Optimization Time per Fold: {best_training_results['optimization_time']:.4f} seconds\")\n",
    "print(f\"Average Training Error (MSE, without regularization) of Optimal Model: {best_training_results['training_error_no_reg']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42486918",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Having identified the optimal set of hyperparameters, the model will be retrained using these **best-performing parameters** on the **entire available training dataset**. This step ensures the model leverages all learning opportunities before its ultimate evaluation. Subsequently, we will assess its true generalization capability by making predictions on the completely unseen test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retraining optimal model on full TRAINING dataset for final evaluation...\n",
      "\n",
      "Final Test Data Shape (from split): (32, 4095), (1, 4095)\n",
      "Test Error (MAPE) of Optimal Model: 20.9393%\n",
      "Test Error (MSE) of Optimal Model: 49.4616\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRetraining optimal model on full TRAINING dataset for final evaluation...\")\n",
    "best_L = best_hyperparameters['num_layers']\n",
    "best_neurons_config = best_hyperparameters['num_neurons_per_layer']\n",
    "# Map activation function name back to the callable objects\n",
    "best_activation_func = g1 if best_hyperparameters['activation_function'] == 'g1' else g2\n",
    "best_activation_prime = dg1_dx if best_hyperparameters['activation_function'] == 'g1' else dg2_dx\n",
    "best_reg_factor = best_hyperparameters['regularization_factor']\n",
    "\n",
    "# Re-initialize for full training on the *entire training set* (X_train_normalized, y_train)\n",
    "W_final_init, b_final_init, v_final_init = initialize_parameters(D_input, best_neurons_config, y_output_dim, best_reg_factor)\n",
    "initial_flat_params_final = unroll_params(W_final_init, b_final_init, v_final_init)\n",
    "\n",
    "W_shapes_final = [W.shape for W in W_final_init]\n",
    "b_shapes_final = [b.shape for b in b_final_init]\n",
    "v_shape_final = v_final_init.shape\n",
    "\n",
    "start_time_final_train = time.time()\n",
    "result_final_train = minimize(\n",
    "    fun=objective_function,\n",
    "    x0=initial_flat_params_final,\n",
    "    args=(X_train_normalized, y_train, W_shapes_final, b_shapes_final, v_shape_final, best_activation_func, best_activation_prime, best_reg_factor, best_L),\n",
    "    method='L-BFGS-B',\n",
    "    jac=True,\n",
    "    options={'disp': False, 'maxiter': 1000}\n",
    ")\n",
    "end_time_final_train = time.time()\n",
    "\n",
    "W_final_trained, b_final_trained, v_final_trained = roll_params(result_final_train.x, W_shapes_final, b_shapes_final, v_shape_final)\n",
    "\n",
    "# Evaluate on the TEST data (X_test_normalized, y_test)\n",
    "print(f\"\\nFinal Test Data Shape (from split): {X_test.shape}, {y_test.shape}\")\n",
    "y_test_pred, _, _ = forward(X_test_normalized, W_final_trained, b_final_trained, v_final_trained, best_activation_func, best_L)\n",
    "test_error_mape = mape(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Error (MAPE) of Optimal Model: {test_error_mape:.4f}%\")\n",
    "\n",
    "# Also calculate MSE test error\n",
    "test_error_mse = mse_loss(y_test, y_test_pred)\n",
    "print(f\"Test Error (MSE) of Optimal Model: {test_error_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc6124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary of Optimal Configuration ---\n",
      "Optimal Number of Layers (L): 2\n",
      "Optimal Number of Neurons per Layer: [32]\n",
      "Optimal Activation Function: g2\n",
      "Optimal Regularization Factor: 0.001\n",
      "Max Iterations for L-BFGS-B (from Best CV Training Results): 2788.2\n",
      "\n",
      "--- Training Set Performance (Optimal Model on Full Training Data) ---\n",
      "Training Error (MAPE): 20.0496%\n",
      "Training Error (MSE, Regularized): 46.8311\n",
      "\n",
      "--- Average Validation Performance (from GridSearchCV Optimal Combination) ---\n",
      "Average Validation Error (MAPE): 20.2029%\n",
      "Average Validation Error (MSE, Regularized): 46.0791 (check if this is truly regularized MSE)\n",
      "\n",
      "--- Test Set Performance (Optimal Model) ---\n",
      "Test Error (MAPE): 20.9393%\n",
      "Test Error (MSE, Regularized): 50.0484\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Summary of Optimal Configuration ---\")\n",
    "print(f\"Optimal Number of Layers (L): {best_L}\")\n",
    "print(f\"Optimal Number of Neurons per Layer: {best_neurons_config}\")\n",
    "print(f\"Optimal Activation Function: {best_activation_func.__name__}\")\n",
    "print(f\"Optimal Regularization Factor: {best_reg_factor}\")\n",
    "best_max_iter = best_training_results['num_iterations']\n",
    "print(f\"Max Iterations for L-BFGS-B (from Best CV Training Results): {best_max_iter}\")\n",
    "\n",
    "\n",
    "# --- Training Set Performance (After Final Retraining) ---\n",
    "print(\"\\n--- Training Set Performance (Optimal Model on Full Training Data) ---\")\n",
    "\n",
    "# Training MAPE (already has y_train_pred from previous evaluation logic if you had it,\n",
    "# but let's re-calculate to be explicit)\n",
    "y_train_pred, _, _ = forward(X_train_normalized, W_final_trained, b_final_trained, v_final_trained, best_activation_func, best_L)\n",
    "train_error_mape = mape(y_train, y_train_pred)\n",
    "print(f\"Training Error (MAPE): {train_error_mape:.4f}%\")\n",
    "\n",
    "# Training MSE (Regularized)\n",
    "# Reuse the objective_function with the final trained parameters and full training data\n",
    "train_error_mse_reg, _ = objective_function(\n",
    "    result_final_train.x, # This is the flattened trained parameters from the last minimize call\n",
    "    X_train_normalized,   # Your original [D,N] training data\n",
    "    y_train,              # Your original [1,N] training data\n",
    "    W_shapes_final, b_shapes_final, v_shape_final, # Shapes derived from final init\n",
    "    best_activation_func, best_activation_prime, best_reg_factor, best_L\n",
    ")\n",
    "print(f\"Training Error (MSE, Regularized): {train_error_mse_reg:.4f}\")\n",
    "\n",
    "\n",
    "# --- Average Validation Performance (From Cross-Validation) ---\n",
    "print(\"\\n--- Average Validation Performance (from GridSearchCV Optimal Combination) ---\")\n",
    "# This `best_mape` variable should contain the average validation MAPE from your GridSearchCV output.\n",
    "print(f\"Average Validation Error (MAPE): {best_mape:.4f}%\")\n",
    "\n",
    "# Average Validation MSE (Regularized)\n",
    "# Note: Unless you explicitly configured GridSearchCV to also track 'neg_mean_squared_error'\n",
    "# (regularized) and stored its 'mean_test_score' for the best combination, this value\n",
    "# is not directly available from `best_mape`. If you did, you would access it from `grid_search.cv_results_`.\n",
    "# For now, we will note this as \"not directly available\" if you only tracked MAPE as the scoring metric.\n",
    "# If your `best_training_results` dict contains an average regularized MSE, you can use it here.\n",
    "try:\n",
    "    avg_val_mse_reg = best_training_results['training_error_no_reg'] # Placeholder - check your dict key carefully\n",
    "    print(f\"Average Validation Error (MSE, Regularized): {avg_val_mse_reg:.4f} (check if this is truly regularized MSE)\")\n",
    "except KeyError:\n",
    "    print(f\"Average Validation Error (MSE, Regularized): (Not directly available from original GridSearchCV output if only MAPE was tracked.)\")\n",
    "\n",
    "\n",
    "# --- Test Set Performance (Optimal Model) ---\n",
    "# (test_error_mape and test_error_mse are already computed in the cell you ran)\n",
    "print(\"\\n--- Test Set Performance (Optimal Model) ---\")\n",
    "print(f\"Test Error (MAPE): {test_error_mape:.4f}%\")\n",
    "\n",
    "# Test MSE (Regularized)\n",
    "# Reuse the objective_function with the final trained parameters and test data\n",
    "test_error_mse_reg, _ = objective_function(\n",
    "    result_final_train.x, # Flattened trained parameters from final minimize call\n",
    "    X_test_normalized,    # Original [D,N] test data\n",
    "    y_test,               # Original [1,N] test data\n",
    "    W_shapes_final, b_shapes_final, v_shape_final, # Shapes derived from final init\n",
    "    best_activation_func, best_activation_prime, best_reg_factor, best_L\n",
    ")\n",
    "print(f\"Test Error (MSE, Regularized): {test_error_mse_reg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
