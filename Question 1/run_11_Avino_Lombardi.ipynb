{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ed43a0",
   "metadata": {},
   "source": [
    "# Part 1 — Multi-Layer Perceptron (Age Regression)\n",
    "\n",
    "| Name            | Student ID | Email                                      |\n",
    "|-----------------|------------|--------------------------------------------|\n",
    "| Valeria Avino   | 1905974    | avino.1905974@studenti.uniroma1.it         |\n",
    "| Marta Lombardi  | 2156537    | lombardi.2156537@studenti.uniroma1.it      |\n",
    "\n",
    "\n",
    "This notebook presents the implementation of a custom Multi-Layer Perceptron (MLP) for a **regression task** using the dataset `AGE REGRESSION.csv`. The goal is to minimize the **L2 regularized loss function** using manually derived gradients and a numerical optimizer provided by `scipy.optimize`.\n",
    "\n",
    "The neural network:\n",
    "- Uses **at least two hidden layers**\n",
    "- Applies **L2 regularization** to the weight matrices\n",
    "- Supports **sigmoid** or **tanh** activation functions\n",
    "- Is optimized via **L-BFGS-B**, without relying on automatic differentiation libraries\n",
    "- Evaluates performance with the **Mean Absolute Percentage Error (MAPE)**\n",
    "- Selects hyperparameters using **k-fold cross-validation**\n",
    "\n",
    "This notebook is structured to:\n",
    "1. Preprocess the dataset\n",
    "2. Define and optimize the MLP model from scratch\n",
    "3. Perform model selection using cross-validation\n",
    "4. Report training, validation, and test results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4828f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable, Tuple, List, Dict, Any\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer \n",
    "\n",
    "\n",
    "# importing auxiliary functions from Functions.py file\n",
    "from Functions_11_Avino_Lombardi import (\n",
    "    forward, backward,\n",
    "    g1, dg1_dx, g2, dg2_dx,\n",
    "    mse_loss, mape,\n",
    "    initialize_parameters, unroll_params, roll_params,\n",
    "    check_gradients_with_central_differences,\n",
    "    objective_function, final_report_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5365d39",
   "metadata": {},
   "source": [
    "### Data loading and splitting\n",
    "After loading the data in our environment, we randomly partition it into training ( $80\\% $) and testing ( $20\\%$ ) sets. This separation is fundamental to evaluate the generalization capability of our trained model on unseen (test) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af05b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape: (20475, 32)\n",
      "Initial y shape: (20475, 1)\n",
      "D_input (number of input features): 32\n",
      "y_output_dim (number of output dimensions): 1\n",
      "\n",
      "X_train shape after split: (32, 16380)\n",
      "y_train shape after split: (1, 16380)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/Val/Documents/GitHub/OMDS-Project/dataset/AGE_PREDICTION.csv')\n",
    "\n",
    "# Separate features (X_full) and target (y_full) from the entire dataset\n",
    "feature_columns = [f'feat_{i}' for i in range(1, 33)]\n",
    "X_full = df[feature_columns].values # Shape will be [N, D]\n",
    "y_full = df['gt'].values.reshape(-1, 1) # Shape will be [N, 1]\n",
    "\n",
    "\n",
    "print(f\"Initial X shape: {X_full.shape}\")\n",
    "print(f\"Initial y shape: {y_full.shape}\")\n",
    "\n",
    "# Get total number of samples\n",
    "N_samples_full = X_full.shape[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=1234)\n",
    "\n",
    "X_train = X_train.T # Transpose to [D, Ntrain]\n",
    "y_train = y_train.T # Transpose to [1, Ntrain]\n",
    "X_test = X_test.T # Transpose to [D, N-Ntrain]\n",
    "y_test = y_test.T # Transpose to [1, N-Ntrain]\n",
    "\n",
    "# Determine D_input (number of input features) - from training data to check shapes\n",
    "D_input = X_train.shape[0]\n",
    "y_output_dim = y_train.shape[0]\n",
    "print(f\"D_input (number of input features): {D_input}\")\n",
    "print(f\"y_output_dim (number of output dimensions): {y_output_dim}\\n\")\n",
    "print(f\"X_train shape after split: {X_train.shape}\") # (N_features, N_train)\n",
    "print(f\"y_train shape after split: {y_train.shape}\") # (1, N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d1cb2",
   "metadata": {},
   "source": [
    "### Feature standardization\n",
    "\n",
    "Before applying any optimization routine, we normalize our data\n",
    "For each feature $x_i$:\n",
    "$$x_i^{\\text{normalized}}=\\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "\n",
    "where $\\mu_i$ and $\\sigma_i$ are the **mean** and **standard deviation** of the $i^{th}$ feature, computed over the **training set**, since at this stage we do not have access to test information. The same transformation is then applied to test data.\n",
    "\n",
    "Standardization ensures __all features contribute equally__ to the loss landscape, thus to the gradient updates, prevents issues like vanishing or exploding gradients due to varying feature scales, and accelerates the convergence of our L-BFGS-B optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3172e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual, but can be done with \"StandardScaler\"\n",
    "# only computed from train data\n",
    "mu = X_train.mean(axis=1, keepdims=True)\n",
    "sigma = X_train.std(axis=1, keepdims=True)\n",
    "\n",
    "# Handle cases where standard deviation might be zero (constant feature)\n",
    "sigma[sigma == 0] = 1e-8 \n",
    "\n",
    "# Apply the transformation to the TRAINING DATA\n",
    "X_train_normalized = (X_train - mu) / sigma\n",
    "\n",
    "# Apply the *SAME* transformation (using mu and sigma from training) to the TEST DATA\n",
    "X_test_normalized = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88984742",
   "metadata": {},
   "source": [
    "Let's check if the gradient was correctly computed by the __Central differences__ approximation:\n",
    "$$f'(x) \\approx \\frac{f(x+\\Delta x)-f(x-\\Delta x)}{2\\Delta x}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4344d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing gradient check...\n",
      "\n",
      "--- Gradient Check (Central Differences) ---\n",
      "Checking 200 parameters...\n",
      "Analytical loss at initial point: 1678.853975\n",
      "\n",
      "Gradient check PASSED! Norm of difference: 1.407312e-06\n",
      "------------------------------------------\n",
      "Gradient check finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming gradient check...\")\n",
    "\n",
    "# subset of training data for gradient check \n",
    "num_samples_for_check = min(1000, X_train_normalized.shape[1])\n",
    "X_check_subset = X_train_normalized[:, :num_samples_for_check]\n",
    "y_check_subset = y_train[:, :num_samples_for_check]\n",
    "\n",
    "# toy set of hyperparameters for the check\n",
    "check_L = 3\n",
    "check_neurons_config = [5, 5]\n",
    "check_activation_func = g1 # also checked with g2\n",
    "check_activation_prime = dg1_dx\n",
    "check_reg_factor = 0.01\n",
    "\n",
    "# initialize parameters\n",
    "W_check_init, b_check_init, v_check_init = initialize_parameters(\n",
    "    D_input, check_neurons_config, y_output_dim, check_reg_factor\n",
    ")\n",
    "initial_flat_params_for_check = unroll_params(W_check_init, b_check_init, v_check_init)\n",
    "\n",
    "W_shapes_for_check = [W.shape for W in W_check_init]\n",
    "b_shapes_for_check = [b.shape for b in b_check_init]\n",
    "v_shape_for_check = v_check_init.shape\n",
    "\n",
    "# gradient check function\n",
    "check_gradients_with_central_differences(\n",
    "    initial_flat_params_for_check,\n",
    "    X_check_subset, y_check_subset,\n",
    "    W_shapes_for_check, b_shapes_for_check, v_shape_for_check,\n",
    "    check_activation_func, check_activation_prime,\n",
    "    check_reg_factor, check_L,\n",
    "    objective_function\n",
    ")\n",
    "\n",
    "print(\"Gradient check finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f9b13",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "We now perform **5-fold cross-validation** on the training data to assess the optimal Multi-Layer Perceptron (MLP) network architecture. This method involves partitioning the training set into k=5 equally sized, disjoint subsets. For each fold $i\\in\\{1,…,5\\}$, the model is trained on the data from the other 4 folds and subsequently evaluated on the held-out fold $F_i$\n",
    "\n",
    "This cross-validation procedure is integrated with a **Grid Search** strategy. The Grid Search exhaustively explores a predefined hyperparameter space **H**, which includes combinations of:\n",
    "\n",
    "- Number of hidden layers: $L-1\\in\\{2,3,4\\}$\n",
    "\n",
    "- Number of neurons per hidden layer\n",
    "\n",
    "- Choice of activation function ( sigmoid or hyperbolic tangent )\n",
    "\n",
    "- Regularization factor ($\\lambda$) for the L2 penalty.\n",
    "\n",
    "For each unique combination of hyperparameters within this grid, a model is trained and evaluated $5$ times (once for each fold). The \"best performance on average on validation sets\" refers to the mean evaluation metric (specifically, the Mean Absolute Percentage Error, MAPE) computed across these $5$ validation folds for that particular hyperparameter combination. The **optimal MLP architecture** is then identified as the combination of hyperparameters from the grid that yields the **lowest average MAPE** across its respective cross-validation folds. This averaging process provides a more robust and reliable estimate of the model's performance by reducing the variance associated with a single train/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b93509",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_k_fold_CV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m      2\u001b[0m hyperparameter_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_neurons_per_layer\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregularization_factor\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# --- Call our custom K-Fold CV function ---\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# max_iter_minimize for the CV phase is set to 1000 for faster grid search.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m best_mape, best_hyperparameters, best_training_results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_k_fold_CV\u001b[49m(\n\u001b[0;32m     16\u001b[0m     X_train_norm\u001b[38;5;241m=\u001b[39mX_train_normalized,\n\u001b[0;32m     17\u001b[0m     y_train_data\u001b[38;5;241m=\u001b[39my_train,\n\u001b[0;32m     18\u001b[0m     D_input\u001b[38;5;241m=\u001b[39mD_input,\n\u001b[0;32m     19\u001b[0m     y_output_dim\u001b[38;5;241m=\u001b[39my_output_dim,\n\u001b[0;32m     20\u001b[0m     hyperparameter_grid\u001b[38;5;241m=\u001b[39mhyperparameter_grid,\n\u001b[0;32m     21\u001b[0m     n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     22\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m,\n\u001b[0;32m     23\u001b[0m     max_iter_minimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m \u001b[38;5;66;03m# Use 500 iterations for the CV search for speed\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Optimal Hyperparameters and Performance (from K-Fold CV) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Number of Layers (L): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_k_fold_CV' is not defined"
     ]
    }
   ],
   "source": [
    "# Define Hyperparameter Search Space\n",
    "hyperparameter_grid = {\n",
    "    'num_layers': [2, 3, 4],\n",
    "    'num_neurons_per_layer': {\n",
    "        2: [[8], [16], [32]], # For L=2\n",
    "        3: [[8, 8], [16, 16], [32, 32]], # For L=3\n",
    "        4: [[8, 8, 8], [16, 16, 16]] # For L=4\n",
    "    },\n",
    "    'activation_function': [(g1, dg1_dx), (g2, dg2_dx)], # Pass tuples of (func, prime_func)\n",
    "    'regularization_factor': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# --- Call our custom K-Fold CV function ---\n",
    "# max_iter_minimize for the CV phase is set to 1000 for faster grid search.\n",
    "best_mape, best_hyperparameters, best_training_results = my_k_fold_CV(\n",
    "    X_train_norm=X_train_normalized,\n",
    "    y_train_data=y_train,\n",
    "    D_input=D_input,\n",
    "    y_output_dim=y_output_dim,\n",
    "    hyperparameter_grid=hyperparameter_grid,\n",
    "    n_splits=5,\n",
    "    random_seed=1234,\n",
    "    max_iter_minimize=500 # Use 500 iterations for the CV search for speed\n",
    ")\n",
    "\n",
    "print(\"\\n--- Optimal Hyperparameters and Performance (from K-Fold CV) ---\")\n",
    "print(f\"Optimal Number of Layers (L): {best_hyperparameters['num_layers']}\")\n",
    "print(f\"Optimal Number of Neurons per Layer (N): {best_hyperparameters['num_neurons_per_layer']}\")\n",
    "print(f\"Optimal Activation Function: {best_hyperparameters['activation_function']}\")\n",
    "print(f\"Optimal Regularization Factor (lambda): {best_hyperparameters['regularization_factor']}\")\n",
    "print(f\"Max Iterations for Optimizer (during CV): {best_hyperparameters['max_iter_minimize']}\")\n",
    "print(f\"Optimization Solver: L-BFGS-B ({best_training_results['optimization_solver']})\")\n",
    "print(f\"Average Number of Iterations for Optimization (during CV): {best_training_results['num_iterations']:.2f}\")\n",
    "print(f\"Average Optimization Time per Fold (during CV): (Removed for speed, not directly tracked)\")\n",
    "print(f\"Average Initial Objective Function Value (during CV): {best_training_results['initial_objective_function_value']:.4e}\")\n",
    "print(f\"Average Final Objective Function Value (during CV): {best_training_results['final_objective_function_value']:.4e}\")\n",
    "print(f\"Average Validation Error (MAPE): {best_training_results['average_validation_mape']:.4f}%\")\n",
    "print(f\"Average Validation Error (MSE, regularized): {best_training_results['average_validation_mse_reg']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d350de44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimal Hyperparameters and Performance (from 5-Fold CV) ---\n",
      "Optimal Number of Layers (L): 2\n",
      "Optimal Number of Neurons per Layer (N): [8]\n",
      "Optimal Activation Function: g2\n",
      "Optimal Regularization Factor (lambda): 0.001\n",
      "Max Iterations for Optimizer (during CV): 500\n",
      "Optimization Solver: L-BFGS-B (STOP: TOTAL NO. of ITERATIONS REACHED LIMIT)\n",
      "Average Number of Iterations for Optimization (during CV): 500.00\n",
      "Average Optimization Time per Fold (during CV): (Removed for speed, not directly tracked)\n",
      "Average Initial Objective Function Value (during CV): 1.6524e+03\n",
      "Average Final Objective Function Value (during CV): 9.4578e+01\n",
      "Average Validation Error (MAPE): 20.3481%\n",
      "Average Validation Error (MSE, regularized): 96.2489\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Optimal Hyperparameters and Performance (from 5-Fold CV) ---\")\n",
    "print(f\"Optimal Number of Layers (L): {best_hyperparameters['num_layers']}\")\n",
    "print(f\"Optimal Number of Neurons per Layer (N): {best_hyperparameters['num_neurons_per_layer']}\")\n",
    "print(f\"Optimal Activation Function: {best_hyperparameters['activation_function']}\")\n",
    "print(f\"Optimal Regularization Factor (lambda): {best_hyperparameters['regularization_factor']}\")\n",
    "print(f\"Max Iterations for Optimizer (during CV): {best_hyperparameters['max_iter_minimize']}\")\n",
    "print(f\"Optimization Solver: L-BFGS-B ({best_training_results['optimization_solver']})\")\n",
    "print(f\"Average Number of Iterations for Optimization (during CV): {best_training_results['num_iterations']:.2f}\")\n",
    "print(f\"Average Optimization Time per Fold (during CV): (Removed for speed, not directly tracked)\")\n",
    "print(f\"Average Initial Objective Function Value (during CV): {best_training_results['initial_objective_function_value']:.4e}\")\n",
    "print(f\"Average Final Objective Function Value (during CV): {best_training_results['final_objective_function_value']:.4e}\")\n",
    "print(f\"Average Validation Error (MAPE): {best_training_results['average_validation_mape']:.4f}%\")\n",
    "print(f\"Average Validation Error (MSE, regularized): {best_training_results['average_validation_mse_reg']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48d274",
   "metadata": {},
   "source": [
    "### Retraining the final model with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1450a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retraining optimal model on full TRAINING dataset for final evaluation...\n",
      "\n",
      "Final model training completed in 30.55 seconds over 1456 iterations.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRetraining optimal model on full TRAINING dataset for final evaluation...\")\n",
    "best_L = best_hyperparameters['num_layers']\n",
    "best_neurons_config = best_hyperparameters['num_neurons_per_layer']\n",
    "best_activation_func = g1 if best_hyperparameters['activation_function'] == 'g1' else g2\n",
    "best_activation_prime = dg1_dx if best_hyperparameters['activation_function'] == 'g1' else dg2_dx\n",
    "best_reg_factor = best_hyperparameters['regularization_factor']\n",
    "\n",
    "# Re-initialize for full training on the *entire training set* (X_train_normalized, y_train)\n",
    "W_final_init, b_final_init, v_final_init = initialize_parameters(D_input, best_neurons_config, y_output_dim, best_reg_factor)\n",
    "initial_flat_params_final = unroll_params(W_final_init, b_final_init, v_final_init)\n",
    "\n",
    "W_shapes_final = [W.shape for W in W_final_init]\n",
    "b_shapes_final = [b.shape for b in b_final_init]\n",
    "v_shape_final = v_final_init.shape\n",
    "\n",
    "start_time_final_train = time.time()\n",
    "result_final_train = minimize(\n",
    "    fun=objective_function,\n",
    "    x0=initial_flat_params_final,\n",
    "    args=(X_train_normalized, y_train, W_shapes_final, b_shapes_final, v_shape_final, best_activation_func, best_activation_prime, best_reg_factor, best_L),\n",
    "    method='L-BFGS-B',\n",
    "    jac=True,\n",
    "    options={'disp': False, 'maxiter': 5000} # here i've incremented max iter for robustness\n",
    ")\n",
    "end_time_final_train = time.time()\n",
    "\n",
    "# Final Trained Parameters\n",
    "W_final_trained, b_final_trained, v_final_trained = roll_params(result_final_train.x, W_shapes_final, b_shapes_final, v_shape_final)\n",
    "final_training_time = end_time_final_train - start_time_final_train\n",
    "final_training_iterations = result_final_train.nit\n",
    "final_train_objective_value = result_final_train.fun # Final regularized MSE on training set\n",
    "print(f\"\\nFinal model training completed in {final_training_time:.2f} seconds over {final_training_iterations} iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dcbf893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Performance:\n",
      "  Final Training Error (MAPE): 20.1753%\n",
      "  Final Training Error (MSE): 93.0971\n",
      "  Final Training Error (MSE, regularized): 94.7941\n"
     ]
    }
   ],
   "source": [
    "# Training Set Performance ---\n",
    "y_train_pred_final, _, _ = forward(X_train_normalized, W_final_trained, b_final_trained, v_final_trained, best_activation_func, best_L)\n",
    "final_train_mape = mape(y_train, y_train_pred_final)\n",
    "train_error_mse = mse_loss(y_train, y_train_pred_final)\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Final Training Error (MAPE): {final_train_mape:.4f}%\")\n",
    "print(f\"  Final Training Error (MSE): {train_error_mse:.4f}\")\n",
    "print(f\"  Final Training Error (MSE, regularized): {final_train_objective_value:.4f}\") # Final regularized MSE on full training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc81fa",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Having identified the optimal set of hyperparameters, the model will be retrained using these **best-performing parameters** on the **entire available training dataset**. This step ensures the model leverages all learning opportunities before its ultimate evaluation. Subsequently, we will assess its true generalization capability by making predictions on the completely unseen test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd1ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Data Shape (from split): (32, 4095), (1, 4095)\n",
      "\n",
      " Test Set Performance:\n",
      "  Final Test Error (MAPE): 20.1206%\n",
      "  Final Test Error (MSE): 94.7598\n",
      "  Final Test Error (MSE, regularized): 96.4569\n"
     ]
    }
   ],
   "source": [
    "# TESTING ON NEVER SEEN BEFORE DATA (X_test_normalized, y_test)\n",
    "print(f\"\\nFinal Test Data Shape (from split): {X_test.shape}, {y_test.shape}\")\n",
    "y_test_pred, _, _ = forward(X_test_normalized, W_final_trained, b_final_trained, v_final_trained, best_activation_func, best_L)\n",
    "test_error_mape = mape(y_test, y_test_pred)\n",
    "test_error_mse = mse_loss(y_test, y_test_pred)\n",
    "\n",
    "test_error_mse_reg, _ = objective_function(\n",
    "    result_final_train.x, \n",
    "    X_test_normalized, y_test, W_shapes_final, b_shapes_final, v_shape_final,\n",
    "    best_activation_func, best_activation_prime, best_reg_factor, best_L\n",
    ")\n",
    "\n",
    "print(\"\\n Test Set Performance:\")\n",
    "print(f\"  Final Test Error (MAPE): {test_error_mape:.4f}%\")\n",
    "print(f\"  Final Test Error (MSE): {test_error_mse:.4f}\")\n",
    "print(f\"  Final Test Error (MSE, regularized): {test_error_mse_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba7a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry-by-Entry Comparison of True vs. Predicted Ages (Test Set):\n",
      "      True Age  Predicted Age\n",
      "49          58      25.854565\n",
      "308         70      39.101132\n",
      "386         60      26.376181\n",
      "811         68      35.880332\n",
      "827         70      31.633644\n",
      "1012        65      32.887125\n",
      "1086        69      27.382353\n",
      "1435        72      35.469229\n",
      "1715        61      29.480984\n",
      "1741        80      47.672859\n",
      "1814        66      30.553193\n",
      "2027        76      34.867095\n",
      "2214        89      54.050493\n",
      "2319        65      30.671877\n",
      "2647        85      45.027950\n",
      "2795        76      31.290346\n",
      "3049        80      43.397095\n",
      "3118        74      42.672235\n",
      "3330        69      27.290921\n",
      "3367        71      38.578848\n",
      "3598        62      31.664357\n",
      "3698        75      44.107233\n",
      "4018        65      32.866114\n"
     ]
    }
   ],
   "source": [
    "true_values = y_test.flatten()\n",
    "predicted_values = y_test_pred.flatten()\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True Age': true_values,\n",
    "    'Predicted Age': predicted_values\n",
    "})\n",
    "\n",
    "print(\"Entry-by-Entry Comparison of True vs. Predicted Ages (Test Set):\")\n",
    "print(comparison_df[abs(comparison_df['True Age']-comparison_df['Predicted Age'] <5 ) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eaa9a3",
   "metadata": {},
   "source": [
    "# Fixed MLP with L= number of hidden layers + class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a86883a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom MLP Regressor Class \n",
    "class myMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    # default values\n",
    "    def __init__(self, D_input: int, y_output_dim: int, num_layers: int = 3, # 2 hidden\n",
    "                 num_neurons: List[int] = [8, 4],\n",
    "                 activation_func_name: str = 'g1',\n",
    "                 regularization_factor: float = 0.001,\n",
    "                 max_iter: int = 5000, print_callback_loss: bool = True):\n",
    "\n",
    "        # Hyperparameters for gridsearch\n",
    "        self.num_layers = num_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation_func_name = activation_func_name\n",
    "        self.regularization_factor = regularization_factor\n",
    "        self.max_iter = max_iter\n",
    "        self.print_callback_loss = print_callback_loss\n",
    "\n",
    "        # Fixed parameters from dataset\n",
    "        self.D_input = D_input\n",
    "        self.y_output_dim = y_output_dim\n",
    "\n",
    "        # Attributes that will be set after fitting (by the fit method)\n",
    "        self.W_list_ = None\n",
    "        self.b_list_ = None\n",
    "        self.v_ = None\n",
    "        self.W_shapes_ = None\n",
    "        self.b_shapes_ = None\n",
    "        self.v_shape_ = None\n",
    "        self.activation_func_ = None \n",
    "        self.activation_prime_ = None\n",
    "        self.n_iterations_ = None\n",
    "        self.optimization_message_ = None\n",
    "        self.final_objective_value_ = None\n",
    "        self._is_invalid_combo = False # Flag to mark invalid hyperparameter combinations\n",
    "\n",
    "    def _get_activation_functions(self):\n",
    "        \"\"\"Maps activation function name (string) to callable functions.\"\"\"\n",
    "        if self.activation_func_name == 'g1':\n",
    "            return g1, dg1_dx\n",
    "        elif self.activation_func_name == 'g2':\n",
    "            return g2, dg2_dx\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function name: {self.activation_func_name}\")\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \n",
    "        X_transposed = X.T # (D, N_samples) \n",
    "        y_transposed = y.T # (1, N_samples)\n",
    "\n",
    "        # Architectural Validation\n",
    "        expected_hidden_layers = self.num_layers - 1 # L total layers, L-1 hidden layers\n",
    "        if len(self.num_neurons) != expected_hidden_layers:\n",
    "            self._is_invalid_combo = True\n",
    "            return self\n",
    "\n",
    "        self._is_invalid_combo = False # Reset flag for valid combinations\n",
    "\n",
    "        # Get activation functions\n",
    "        self.activation_func_, self.activation_prime_ = self._get_activation_functions()\n",
    "\n",
    "        # Initialize parameters\n",
    "        W_init, b_init, v_init = initialize_parameters(\n",
    "            self.D_input, self.num_neurons, self.y_output_dim, self.regularization_factor\n",
    "        )\n",
    "        initial_flat_params = unroll_params(W_init, b_init, v_init)\n",
    "\n",
    "        # Store shapes of parameters for rolling/unrolling inside objective_function\n",
    "        self.W_shapes_ = [W.shape for W in W_init]\n",
    "        self.b_shapes_ = [b.shape for b in b_init]\n",
    "        self.v_shape_ = v_init.shape\n",
    "\n",
    "        # Callback function to print mse loss in training\n",
    "        iteration_count = 0 \n",
    "\n",
    "        def callback_function(current_flat_params):\n",
    "            nonlocal iteration_count \n",
    "            iteration_count += 1\n",
    "\n",
    "            if iteration_count % 10 == 0:\n",
    "                W_list_cb, b_list_cb, v_cb = roll_params(\n",
    "                    current_flat_params, self.W_shapes_, self.b_shapes_, self.v_shape_\n",
    "                )\n",
    "                # Perform forward pass to get y_pred\n",
    "                y_pred_cb, _, _ = forward(\n",
    "                    X_transposed, W_list_cb, b_list_cb, v_cb, self.activation_func_, self.num_layers\n",
    "                )\n",
    "                # Calculate non-regularized MSE loss\n",
    "                non_reg_loss = mse_loss(y_transposed, y_pred_cb)\n",
    "                print(f\"  Iteration {iteration_count}: Non-regularized MSE Loss = {non_reg_loss:.6f}\")\n",
    "\n",
    "        callback_arg = callback_function if self.print_callback_loss else None\n",
    "\n",
    "        result = minimize(\n",
    "            fun=objective_function,\n",
    "            x0=initial_flat_params,\n",
    "            args=(X_transposed, y_transposed, self.W_shapes_, self.b_shapes_, self.v_shape_,\n",
    "                  self.activation_func_, self.activation_prime_, self.regularization_factor, self.num_layers),\n",
    "            method='L-BFGS-B',\n",
    "            jac=True,\n",
    "            options={'disp': False, 'maxiter': self.max_iter},\n",
    "            callback=callback_arg # Pass the callback here\n",
    "        )\n",
    "\n",
    "        # Store the optimized parameters and optimization details\n",
    "        self.W_list_, self.b_list_, self.v_ = roll_params(\n",
    "            result.x, self.W_shapes_, self.b_shapes_, self.v_shape_\n",
    "        )\n",
    "        self.n_iterations_ = result.nit\n",
    "        self.optimization_message_ = result.message\n",
    "        self.final_objective_value_ = result.fun # final (regularized) loss after optimization\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # If combination flagged invalid during fit, raise an error\n",
    "        if self._is_invalid_combo:\n",
    "            raise NotFittedError(\"This estimator was skipped due to an invalid hyperparameter combination during fit.\")\n",
    "        # Check if the model has actually been trained\n",
    "        if self.W_list_ is None:\n",
    "            raise NotFittedError(\"Model has not been trained yet. Call .fit() first.\")\n",
    "\n",
    "        # Transpose X for forward function\n",
    "        X_transposed = X.T\n",
    "        # Perform forward pass with the TRAINED parameters to get predictions\n",
    "        y_pred, _, _ = forward(X_transposed, self.W_list_, self.b_list_, self.v_, self.activation_func_, self.num_layers)\n",
    "        # Transpose prediction back to (N_samples,1) for sklearn compatibility\n",
    "        return y_pred.T.flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a847060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sklearn shape (for sklearn): (16380, 32)\n",
      "y_train_sklearn shape (for sklearn): (16380, 1)\n",
      "\n",
      "Starting GridSearchCV based Hyperparameter Tuning...\n",
      "\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "GridSearchCV completed.\n",
      "GridSearchCV took 2322.47 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Custom MAPE Scorer for GridSearchCV \n",
    "mape_scorer = make_scorer(lambda y_true, y_pred: -mape(y_true.reshape(1,-1), y_pred.reshape(1,-1)), greater_is_better=True)\n",
    "\n",
    "# X_train_normalized and y_train are now (D, N) and (1, N) \n",
    "# For scikit-learn, they need to be (N, D) and (N, 1).\n",
    "X_train_sklearn = X_train_normalized.T\n",
    "y_train_sklearn = y_train.T\n",
    "\n",
    "X_test_sklearn = X_test_normalized.T\n",
    "y_test_sklearn = y_test.T \n",
    "\n",
    "print(f\"X_train_sklearn shape (for sklearn): {X_train_sklearn.shape}\")\n",
    "print(f\"y_train_sklearn shape (for sklearn): {y_train_sklearn.shape}\\n\")\n",
    "\n",
    "\n",
    "# Hyperparameter Search Space for GridSearchCV\n",
    "param_grid_full = {\n",
    "    'num_layers': [3, 4, 5], # Total layers\n",
    "    'num_neurons': [\n",
    "        [16, 8], [32, 16], [32, 32], \n",
    "        [16, 16, 16], [32, 16, 32], [32, 32, 32],\n",
    "        [16, 8, 16, 8], [32, 16, 32, 16] \n",
    "    ],\n",
    "    'activation_func_name': ['g1', 'g2'], \n",
    "    'regularization_factor': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Keep only VALID combinations\n",
    "filtered_param_grid = []\n",
    "for L_val in param_grid_full['num_layers']:\n",
    "    # Number of hidden layers is L_val - 1 (L is total layers, including output)\n",
    "    expected_hidden_layers = L_val - 1\n",
    "    for N_list_val in param_grid_full['num_neurons']:\n",
    "        if len(N_list_val) == expected_hidden_layers:\n",
    "            for act_name_val in param_grid_full['activation_func_name']:\n",
    "                for reg_f_val in param_grid_full['regularization_factor']:\n",
    "                    filtered_param_grid.append({\n",
    "                        'num_layers': [L_val],\n",
    "                        'num_neurons': [N_list_val],\n",
    "                        'activation_func_name': [act_name_val],\n",
    "                        'regularization_factor': [reg_f_val],\n",
    "                    })\n",
    "\n",
    "# Instantiate the MLP Regressor for cv, with a reduced maxiter for speed\n",
    "mlp_estimator = myMLPRegressor(D_input=D_input, y_output_dim=y_output_dim, max_iter=500, print_callback_loss=False) # no loss prints here\n",
    "\n",
    "# Setup KFold for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "print(\"Starting GridSearchCV based Hyperparameter Tuning...\\n\")\n",
    "\n",
    "# Set up GridSearchCV \n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp_estimator,\n",
    "    param_grid=filtered_param_grid, \n",
    "    cv=kf,\n",
    "    scoring=mape_scorer, \n",
    "    verbose=2, \n",
    "    n_jobs=-1,\n",
    "    error_score=np.nan ,\n",
    "    refit=True # for initial loss calculation\n",
    ")\n",
    "\n",
    "# Run the grid search \n",
    "start_time_grid_search = time.time()\n",
    "grid_search.fit(X_train_sklearn, y_train_sklearn)\n",
    "end_time_grid_search = time.time()\n",
    "grid_search_time = end_time_grid_search - start_time_grid_search\n",
    "\n",
    "print(\"\\nGridSearchCV completed.\")\n",
    "print(f\"GridSearchCV took {grid_search_time:.2f} seconds to complete.\")\n",
    "\n",
    "# Results = Best hyperparameters\n",
    "best_overall_params = grid_search.best_params_\n",
    "best_overall_mape_score = -grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e60f5",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Having identified the optimal set of hyperparameters, the model will be retrained using these **best-performing parameters** on the **entire available training dataset**. This step ensures the model leverages all learning opportunities before its ultimate evaluation. Subsequently, we will assess its true generalization capability by making predictions on the completely **unseen test** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7034af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retraining Optimal Model on Full TRAINING Dataset (max_iter=5000) ---\n",
      "\n",
      "Final training uses Optimal Configuration:\n",
      "  Layers (L): 4\n",
      "  Neurons per Layer: [32, 16, 32]\n",
      "  Activation Function: g2\n",
      "  Regularization Factor: 0.01\n",
      "  Max Iterations for L-BFGS-B (Final Train): 5000\n",
      "  Iteration 10: Non-regularized MSE Loss = 114.039038\n",
      "  Iteration 20: Non-regularized MSE Loss = 97.124642\n",
      "  Iteration 30: Non-regularized MSE Loss = 95.988423\n",
      "  Iteration 40: Non-regularized MSE Loss = 95.702783\n",
      "  Iteration 50: Non-regularized MSE Loss = 95.648880\n",
      "  Iteration 60: Non-regularized MSE Loss = 95.335569\n",
      "  Iteration 70: Non-regularized MSE Loss = 95.148132\n",
      "  Iteration 80: Non-regularized MSE Loss = 95.020283\n",
      "  Iteration 90: Non-regularized MSE Loss = 94.966386\n",
      "  Iteration 100: Non-regularized MSE Loss = 94.972889\n",
      "  Iteration 110: Non-regularized MSE Loss = 94.876734\n",
      "  Iteration 120: Non-regularized MSE Loss = 94.807525\n",
      "  Iteration 130: Non-regularized MSE Loss = 94.720381\n",
      "  Iteration 140: Non-regularized MSE Loss = 94.708077\n",
      "  Iteration 150: Non-regularized MSE Loss = 94.694754\n",
      "  Iteration 160: Non-regularized MSE Loss = 94.685182\n",
      "  Iteration 170: Non-regularized MSE Loss = 94.648626\n",
      "  Iteration 180: Non-regularized MSE Loss = 94.582463\n",
      "  Iteration 190: Non-regularized MSE Loss = 94.567415\n",
      "  Iteration 200: Non-regularized MSE Loss = 94.531942\n",
      "  Iteration 210: Non-regularized MSE Loss = 94.487906\n",
      "  Iteration 220: Non-regularized MSE Loss = 94.454632\n",
      "  Iteration 230: Non-regularized MSE Loss = 94.431525\n",
      "  Iteration 240: Non-regularized MSE Loss = 94.403857\n",
      "  Iteration 250: Non-regularized MSE Loss = 94.361869\n",
      "  Iteration 260: Non-regularized MSE Loss = 94.332701\n",
      "  Iteration 270: Non-regularized MSE Loss = 94.307749\n",
      "  Iteration 280: Non-regularized MSE Loss = 94.268062\n",
      "  Iteration 290: Non-regularized MSE Loss = 94.218305\n",
      "  Iteration 300: Non-regularized MSE Loss = 94.180831\n",
      "  Iteration 310: Non-regularized MSE Loss = 94.147804\n",
      "  Iteration 320: Non-regularized MSE Loss = 94.123263\n",
      "  Iteration 330: Non-regularized MSE Loss = 94.099845\n",
      "  Iteration 340: Non-regularized MSE Loss = 94.103005\n",
      "  Iteration 350: Non-regularized MSE Loss = 94.054739\n",
      "  Iteration 360: Non-regularized MSE Loss = 94.014195\n",
      "  Iteration 370: Non-regularized MSE Loss = 93.993323\n",
      "  Iteration 380: Non-regularized MSE Loss = 93.995005\n",
      "  Iteration 390: Non-regularized MSE Loss = 93.962960\n",
      "  Iteration 400: Non-regularized MSE Loss = 93.951594\n",
      "  Iteration 410: Non-regularized MSE Loss = 93.952417\n",
      "  Iteration 420: Non-regularized MSE Loss = 93.942268\n",
      "  Iteration 430: Non-regularized MSE Loss = 93.934836\n",
      "  Iteration 440: Non-regularized MSE Loss = 93.906546\n",
      "  Iteration 450: Non-regularized MSE Loss = 93.889217\n",
      "  Iteration 460: Non-regularized MSE Loss = 93.880098\n",
      "  Iteration 470: Non-regularized MSE Loss = 93.879575\n",
      "  Iteration 480: Non-regularized MSE Loss = 93.866206\n",
      "  Iteration 490: Non-regularized MSE Loss = 93.867440\n",
      "  Iteration 500: Non-regularized MSE Loss = 93.856326\n",
      "  Iteration 510: Non-regularized MSE Loss = 93.848169\n",
      "  Iteration 520: Non-regularized MSE Loss = 93.845345\n",
      "  Iteration 530: Non-regularized MSE Loss = 93.842924\n",
      "  Iteration 540: Non-regularized MSE Loss = 93.835115\n",
      "  Iteration 550: Non-regularized MSE Loss = 93.827326\n",
      "  Iteration 560: Non-regularized MSE Loss = 93.822189\n",
      "  Iteration 570: Non-regularized MSE Loss = 93.815516\n",
      "  Iteration 580: Non-regularized MSE Loss = 93.812530\n",
      "  Iteration 590: Non-regularized MSE Loss = 93.812038\n",
      "  Iteration 600: Non-regularized MSE Loss = 93.809506\n",
      "  Iteration 610: Non-regularized MSE Loss = 93.808720\n",
      "  Iteration 620: Non-regularized MSE Loss = 93.806456\n",
      "  Iteration 630: Non-regularized MSE Loss = 93.803693\n",
      "  Iteration 640: Non-regularized MSE Loss = 93.798229\n",
      "  Iteration 650: Non-regularized MSE Loss = 93.794099\n",
      "  Iteration 660: Non-regularized MSE Loss = 93.786624\n",
      "  Iteration 670: Non-regularized MSE Loss = 93.783440\n",
      "  Iteration 680: Non-regularized MSE Loss = 93.781325\n",
      "  Iteration 690: Non-regularized MSE Loss = 93.778741\n",
      "  Iteration 700: Non-regularized MSE Loss = 93.772421\n",
      "  Iteration 710: Non-regularized MSE Loss = 93.771554\n",
      "  Iteration 720: Non-regularized MSE Loss = 93.768766\n",
      "  Iteration 730: Non-regularized MSE Loss = 93.765047\n",
      "  Iteration 740: Non-regularized MSE Loss = 93.761733\n",
      "  Iteration 750: Non-regularized MSE Loss = 93.756727\n",
      "  Iteration 760: Non-regularized MSE Loss = 93.751314\n",
      "  Iteration 770: Non-regularized MSE Loss = 93.751038\n",
      "  Iteration 780: Non-regularized MSE Loss = 93.751011\n",
      "  Iteration 790: Non-regularized MSE Loss = 93.750537\n",
      "  Iteration 800: Non-regularized MSE Loss = 93.751227\n",
      "  Iteration 810: Non-regularized MSE Loss = 93.751770\n",
      "  Iteration 820: Non-regularized MSE Loss = 93.751173\n",
      "  Iteration 830: Non-regularized MSE Loss = 93.749810\n",
      "  Iteration 840: Non-regularized MSE Loss = 93.749231\n",
      "  Iteration 850: Non-regularized MSE Loss = 93.750709\n",
      "  Iteration 860: Non-regularized MSE Loss = 93.751179\n",
      "  Iteration 870: Non-regularized MSE Loss = 93.750225\n",
      "  Iteration 880: Non-regularized MSE Loss = 93.749452\n",
      "  Iteration 890: Non-regularized MSE Loss = 93.747266\n",
      "  Iteration 900: Non-regularized MSE Loss = 93.744976\n",
      "  Iteration 910: Non-regularized MSE Loss = 93.744560\n",
      "  Iteration 920: Non-regularized MSE Loss = 93.744300\n",
      "  Iteration 930: Non-regularized MSE Loss = 93.745911\n",
      "  Iteration 940: Non-regularized MSE Loss = 93.747513\n",
      "  Iteration 950: Non-regularized MSE Loss = 93.747493\n",
      "  Iteration 960: Non-regularized MSE Loss = 93.747159\n",
      "  Iteration 970: Non-regularized MSE Loss = 93.745457\n",
      "  Iteration 980: Non-regularized MSE Loss = 93.744773\n",
      "  Iteration 990: Non-regularized MSE Loss = 93.744481\n",
      "  Iteration 1000: Non-regularized MSE Loss = 93.744941\n",
      "  Iteration 1010: Non-regularized MSE Loss = 93.745897\n",
      "  Iteration 1020: Non-regularized MSE Loss = 93.747581\n",
      "  Iteration 1030: Non-regularized MSE Loss = 93.747834\n",
      "  Iteration 1040: Non-regularized MSE Loss = 93.747779\n",
      "  Iteration 1050: Non-regularized MSE Loss = 93.749207\n",
      "  Iteration 1060: Non-regularized MSE Loss = 93.751517\n",
      "  Iteration 1070: Non-regularized MSE Loss = 93.751614\n",
      "  Iteration 1080: Non-regularized MSE Loss = 93.750388\n",
      "  Iteration 1090: Non-regularized MSE Loss = 93.748856\n",
      "  Iteration 1100: Non-regularized MSE Loss = 93.747158\n",
      "  Iteration 1110: Non-regularized MSE Loss = 93.746725\n",
      "  Iteration 1120: Non-regularized MSE Loss = 93.745794\n",
      "  Iteration 1130: Non-regularized MSE Loss = 93.745052\n",
      "  Iteration 1140: Non-regularized MSE Loss = 93.744700\n",
      "  Iteration 1150: Non-regularized MSE Loss = 93.744617\n",
      "  Iteration 1160: Non-regularized MSE Loss = 93.743934\n",
      "  Iteration 1170: Non-regularized MSE Loss = 93.743418\n",
      "  Iteration 1180: Non-regularized MSE Loss = 93.742752\n",
      "  Iteration 1190: Non-regularized MSE Loss = 93.742215\n",
      "  Iteration 1200: Non-regularized MSE Loss = 93.742162\n",
      "  Iteration 1210: Non-regularized MSE Loss = 93.741845\n",
      "  Iteration 1220: Non-regularized MSE Loss = 93.742626\n",
      "  Iteration 1230: Non-regularized MSE Loss = 93.743155\n",
      "  Iteration 1240: Non-regularized MSE Loss = 93.743898\n",
      "  Iteration 1250: Non-regularized MSE Loss = 93.745083\n",
      "  Iteration 1260: Non-regularized MSE Loss = 93.745332\n",
      "  Iteration 1270: Non-regularized MSE Loss = 93.745269\n",
      "  Iteration 1280: Non-regularized MSE Loss = 93.744510\n",
      "  Iteration 1290: Non-regularized MSE Loss = 93.744049\n",
      "  Iteration 1300: Non-regularized MSE Loss = 93.742895\n",
      "  Iteration 1310: Non-regularized MSE Loss = 93.741789\n",
      "  Iteration 1320: Non-regularized MSE Loss = 93.741424\n",
      "  Iteration 1330: Non-regularized MSE Loss = 93.740960\n",
      "  Iteration 1340: Non-regularized MSE Loss = 93.740382\n",
      "  Iteration 1350: Non-regularized MSE Loss = 93.739644\n",
      "  Iteration 1360: Non-regularized MSE Loss = 93.739065\n",
      "  Iteration 1370: Non-regularized MSE Loss = 93.738169\n",
      "  Iteration 1380: Non-regularized MSE Loss = 93.737672\n",
      "  Iteration 1390: Non-regularized MSE Loss = 93.736404\n",
      "  Iteration 1400: Non-regularized MSE Loss = 93.736083\n",
      "  Iteration 1410: Non-regularized MSE Loss = 93.735570\n",
      "  Iteration 1420: Non-regularized MSE Loss = 93.734712\n",
      "  Iteration 1430: Non-regularized MSE Loss = 93.733783\n",
      "  Iteration 1440: Non-regularized MSE Loss = 93.734180\n",
      "  Iteration 1450: Non-regularized MSE Loss = 93.735014\n",
      "  Iteration 1460: Non-regularized MSE Loss = 93.735411\n",
      "  Iteration 1470: Non-regularized MSE Loss = 93.736097\n",
      "  Iteration 1480: Non-regularized MSE Loss = 93.736932\n",
      "  Iteration 1490: Non-regularized MSE Loss = 93.738087\n",
      "  Iteration 1500: Non-regularized MSE Loss = 93.738590\n",
      "  Iteration 1510: Non-regularized MSE Loss = 93.738334\n",
      "  Iteration 1520: Non-regularized MSE Loss = 93.737942\n",
      "  Iteration 1530: Non-regularized MSE Loss = 93.737285\n",
      "  Iteration 1540: Non-regularized MSE Loss = 93.736680\n",
      "  Iteration 1550: Non-regularized MSE Loss = 93.736488\n",
      "  Iteration 1560: Non-regularized MSE Loss = 93.736470\n",
      "  Iteration 1570: Non-regularized MSE Loss = 93.736578\n",
      "  Iteration 1580: Non-regularized MSE Loss = 93.736525\n",
      "  Iteration 1590: Non-regularized MSE Loss = 93.736299\n",
      "  Iteration 1600: Non-regularized MSE Loss = 93.736158\n",
      "  Iteration 1610: Non-regularized MSE Loss = 93.735719\n",
      "  Iteration 1620: Non-regularized MSE Loss = 93.735340\n",
      "  Iteration 1630: Non-regularized MSE Loss = 93.735530\n",
      "  Iteration 1640: Non-regularized MSE Loss = 93.735944\n",
      "  Iteration 1650: Non-regularized MSE Loss = 93.737262\n",
      "  Iteration 1660: Non-regularized MSE Loss = 93.739618\n",
      "  Iteration 1670: Non-regularized MSE Loss = 93.741662\n",
      "  Iteration 1680: Non-regularized MSE Loss = 93.741865\n",
      "  Iteration 1690: Non-regularized MSE Loss = 93.742638\n",
      "  Iteration 1700: Non-regularized MSE Loss = 93.742988\n",
      "  Iteration 1710: Non-regularized MSE Loss = 93.744217\n",
      "  Iteration 1720: Non-regularized MSE Loss = 93.744683\n",
      "  Iteration 1730: Non-regularized MSE Loss = 93.744949\n",
      "  Iteration 1740: Non-regularized MSE Loss = 93.745020\n",
      "  Iteration 1750: Non-regularized MSE Loss = 93.744751\n",
      "  Iteration 1760: Non-regularized MSE Loss = 93.743984\n",
      "  Iteration 1770: Non-regularized MSE Loss = 93.743915\n",
      "  Iteration 1780: Non-regularized MSE Loss = 93.744081\n",
      "  Iteration 1790: Non-regularized MSE Loss = 93.744368\n",
      "  Iteration 1800: Non-regularized MSE Loss = 93.744896\n",
      "  Iteration 1810: Non-regularized MSE Loss = 93.745335\n",
      "  Iteration 1820: Non-regularized MSE Loss = 93.746115\n",
      "  Iteration 1830: Non-regularized MSE Loss = 93.747890\n",
      "  Iteration 1840: Non-regularized MSE Loss = 93.750158\n",
      "  Iteration 1850: Non-regularized MSE Loss = 93.752988\n",
      "  Iteration 1860: Non-regularized MSE Loss = 93.753098\n",
      "  Iteration 1870: Non-regularized MSE Loss = 93.754077\n",
      "  Iteration 1880: Non-regularized MSE Loss = 93.755095\n",
      "  Iteration 1890: Non-regularized MSE Loss = 93.755197\n",
      "  Iteration 1900: Non-regularized MSE Loss = 93.755264\n",
      "  Iteration 1910: Non-regularized MSE Loss = 93.755589\n",
      "  Iteration 1920: Non-regularized MSE Loss = 93.755753\n",
      "  Iteration 1930: Non-regularized MSE Loss = 93.755777\n",
      "  Iteration 1940: Non-regularized MSE Loss = 93.755525\n",
      "  Iteration 1950: Non-regularized MSE Loss = 93.754808\n",
      "  Iteration 1960: Non-regularized MSE Loss = 93.754459\n",
      "  Iteration 1970: Non-regularized MSE Loss = 93.753955\n",
      "  Iteration 1980: Non-regularized MSE Loss = 93.753294\n",
      "  Iteration 1990: Non-regularized MSE Loss = 93.753019\n",
      "  Iteration 2000: Non-regularized MSE Loss = 93.753399\n",
      "  Iteration 2010: Non-regularized MSE Loss = 93.754475\n",
      "  Iteration 2020: Non-regularized MSE Loss = 93.755159\n",
      "  Iteration 2030: Non-regularized MSE Loss = 93.754938\n",
      "  Iteration 2040: Non-regularized MSE Loss = 93.754721\n",
      "  Iteration 2050: Non-regularized MSE Loss = 93.754529\n",
      "  Iteration 2060: Non-regularized MSE Loss = 93.754215\n",
      "  Iteration 2070: Non-regularized MSE Loss = 93.754036\n",
      "  Iteration 2080: Non-regularized MSE Loss = 93.753780\n",
      "  Iteration 2090: Non-regularized MSE Loss = 93.753513\n",
      "  Iteration 2100: Non-regularized MSE Loss = 93.753521\n",
      "  Iteration 2110: Non-regularized MSE Loss = 93.754041\n",
      "  Iteration 2120: Non-regularized MSE Loss = 93.754387\n",
      "  Iteration 2130: Non-regularized MSE Loss = 93.754407\n",
      "  Iteration 2140: Non-regularized MSE Loss = 93.754389\n",
      "  Iteration 2150: Non-regularized MSE Loss = 93.754038\n",
      "  Iteration 2160: Non-regularized MSE Loss = 93.753398\n",
      "  Iteration 2170: Non-regularized MSE Loss = 93.753204\n",
      "  Iteration 2180: Non-regularized MSE Loss = 93.753140\n",
      "  Iteration 2190: Non-regularized MSE Loss = 93.753383\n",
      "  Iteration 2200: Non-regularized MSE Loss = 93.753641\n",
      "  Iteration 2210: Non-regularized MSE Loss = 93.753577\n",
      "  Iteration 2220: Non-regularized MSE Loss = 93.753843\n",
      "  Iteration 2230: Non-regularized MSE Loss = 93.754081\n",
      "  Iteration 2240: Non-regularized MSE Loss = 93.754134\n",
      "\n",
      "Final model training completed in 200.95 seconds over 2243 iterations.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Retraining Optimal Model on Full TRAINING Dataset (max_iter=5000) ---\")\n",
    "\n",
    "# Extract final parameters from best_overall_params\n",
    "final_L = best_overall_params['num_layers']\n",
    "final_neurons_config = best_overall_params['num_neurons']\n",
    "final_activation_name = best_overall_params['activation_func_name']\n",
    "final_reg_factor = best_overall_params['regularization_factor']\n",
    "final_train_max_iter = 5000\n",
    "\n",
    "final_activation_func = g1 if final_activation_name == 'g1' else g2\n",
    "final_activation_prime = dg1_dx if final_activation_name == 'g1' else dg2_dx\n",
    "\n",
    "print(f\"\\nFinal training uses Optimal Configuration:\")\n",
    "print(f\"  Layers (L): {final_L}\")\n",
    "print(f\"  Neurons per Layer: {final_neurons_config}\")\n",
    "print(f\"  Activation Function: {final_activation_func.__name__}\")\n",
    "print(f\"  Regularization Factor: {final_reg_factor}\")\n",
    "print(f\"  Max Iterations for L-BFGS-B (Final Train): {final_train_max_iter}\")\n",
    "\n",
    "\n",
    "final_model = myMLPRegressor(\n",
    "    D_input=D_input,\n",
    "    y_output_dim=y_output_dim, \n",
    "    num_layers=final_L,\n",
    "    num_neurons=final_neurons_config,\n",
    "    activation_func_name=final_activation_name,\n",
    "    regularization_factor=final_reg_factor,\n",
    "    max_iter=final_train_max_iter,\n",
    "    print_callback_loss=True # callback printing to track mse loss behaviour\n",
    ")\n",
    "\n",
    "# Calculate Initial Training Error (Regularized MSE & MAPE) \n",
    "W_final_init, b_final_init, v_final_init = initialize_parameters(\n",
    "    D_input, final_neurons_config, y_output_dim, final_reg_factor\n",
    ")\n",
    "initial_flat_params_final_model = unroll_params(W_final_init, b_final_init, v_final_init)\n",
    "\n",
    "W_shapes_final_model = [W.shape for W in W_final_init]\n",
    "b_shapes_final_model = [b.shape for b in b_final_init] \n",
    "v_shape_final_model = v_final_init.shape\n",
    "\n",
    "y_train_pred_initial_final_model, _, _ = forward(\n",
    "    X_train_normalized, W_final_init, b_final_init, v_final_init, final_activation_func, final_L\n",
    ")\n",
    "initial_train_mape_final_model = mape(y_train, y_train_pred_initial_final_model)\n",
    "initial_train_mse_reg_final_model, _ = objective_function(\n",
    "    initial_flat_params_final_model,\n",
    "    X_train_normalized, y_train, W_shapes_final_model, b_shapes_final_model, v_shape_final_model,\n",
    "    final_activation_func, final_activation_prime, final_reg_factor, final_L\n",
    ")\n",
    "\n",
    "# Final Training on Full Training Data by calling .fit() on myMLPRegressor instance\n",
    "start_time_final_train = time.time()\n",
    "final_model.fit(X_train_sklearn, y_train_sklearn) \n",
    "end_time_final_train = time.time()\n",
    "\n",
    "# predict on train to get MAPE\n",
    "y_train_pred_final = final_model.predict(X_train_sklearn)\n",
    "final_train_mape = mape(y_train, y_train_pred_final)\n",
    "\n",
    "# Extract results directly from final_model's attributes\n",
    "final_training_time = end_time_final_train - start_time_final_train\n",
    "final_training_iterations = final_model.n_iterations_\n",
    "final_train_objective_value = final_model.final_objective_value_\n",
    "result_final_train_message = final_model.optimization_message_ # message from the fitted model\n",
    "\n",
    "print(f\"\\nFinal model training completed in {final_training_time:.2f} seconds over {final_training_iterations} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a191056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Test Set Performance ---\n",
      "  Test Error (MAPE): 20.1203%\n",
      "  Test Error (MSE, regularized): 98.5102\n",
      "  Test Error (MSE, non-regularized): 94.6767\n"
     ]
    }
   ],
   "source": [
    "# Extract trained parameters from final_model\n",
    "W_final_trained = final_model.W_list_\n",
    "b_final_trained = final_model.b_list_\n",
    "v_final_trained = final_model.v_\n",
    "\n",
    "print(\"\\n--- Calculating Test Set Performance ---\")\n",
    "# Make predictions on the test set using the final trained model\n",
    "y_test_pred, _, _ = forward(X_test_normalized, W_final_trained, b_final_trained, v_final_trained, final_activation_func, final_L)\n",
    "\n",
    "# MAPE on the test set\n",
    "test_error_mape = mape(y_test, y_test_pred)\n",
    "\n",
    "# Non regularized MSE\n",
    "test_mse_no_reg = mse_loss(y_test, y_test_pred)\n",
    "\n",
    "# vs Non regulatized train MSE \n",
    "final_train_mse_no_reg = mse_loss(y_train, y_train_pred_final)\n",
    "\n",
    "# Regularized MSE on the test set\n",
    "test_error_mse_reg, _ = objective_function(\n",
    "    unroll_params(W_final_trained, b_final_trained, v_final_trained), \n",
    "    X_test_normalized, y_test, # Test data\n",
    "    W_shapes_final_model, b_shapes_final_model, v_shape_final_model,\n",
    "    final_activation_func, final_activation_prime, final_reg_factor, final_L # Hyperparams\n",
    ")\n",
    "print(f\"  Test Error (MAPE): {test_error_mape:.4f}%\")\n",
    "print(f\"  Test Error (MSE, regularized): {test_error_mse_reg:.4f}\")\n",
    "print(f\"  Test Error (MSE, non-regularized): {test_mse_no_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8ef9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comprehensive Performance Metrics for Final Report ---\n",
      "\n",
      "1. Optimal Model Configuration:\n",
      "  Non-linearity (Activation Function): g2\n",
      "  Total Number of Layers (L): 4\n",
      "  Neurons per Layer (Nl): [32, 16, 32]\n",
      "  Regularization Factor (λ): 0.01\n",
      "\n",
      "2. Optimization Routine Details (for Final Training):\n",
      "  Optimization Routine: L-BFGS-B\n",
      "  Max Number of Iterations Parameter: 5000\n",
      "  Returned Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Number of Iterations Performed: 2243\n",
      "  Starting Value of Objective Function: 1.7146e+03\n",
      "  Final Value of Objective Function: 9.7588e+01\n",
      "\n",
      "3. Training Set Performance:\n",
      "  Initial Training Error (MAPE): 5462.4904%\n",
      "  Initial Training Error (MSE, regularized): 1714.6084\n",
      "  Final Training Error (MAPE): 20.2494%\n",
      "  Final Training Error (MSE, regularized): 97.5876\n",
      "  Final Training Error (MSE, **non-regularized**): 93.7542\n",
      "\n",
      "4. Validation Set Performance (Average from K-Fold CV):\n",
      "  Average Validation Error (MAPE): 20.3744%\n",
      "\n",
      "5. Test Set Performance:\n",
      "  Final Test Error (MAPE): 20.1203%\n",
      "  Final Test Error (MSE, regularized): 98.5102\n",
      "  Final Test Error (MSE, **non-regularized**): 94.6767\n",
      "\n",
      "Performance Metrics Summary Table for Report (Figure 1 Data):\n",
      "Metric                    Training (Final)     Validation (Avg)     Test (Final)        \n",
      "-------------------------------------------------------------------------------------\n",
      "MAPE (%)                  20.2494%                  20.1203%                 20.3744%                  \n",
      "Regularized MSE           97.5876                   98.5102                  \n",
      "MSE (no reg)              93.7542                   94.6767                  \n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# auxiliary function to display metrics for report\n",
    "final_report_metrics(\n",
    "    final_L=final_L,\n",
    "    final_neurons_config=final_neurons_config,\n",
    "    final_activation_name=final_activation_name,\n",
    "    final_reg_factor=final_reg_factor,\n",
    "    final_train_max_iter=final_train_max_iter,\n",
    "    final_training_iterations=final_training_iterations,\n",
    "    initial_train_mse_reg_final_model=initial_train_mse_reg_final_model,\n",
    "    final_train_objective_value=final_train_objective_value,\n",
    "    initial_train_mape_final_model=initial_train_mape_final_model,\n",
    "    final_train_mape=final_train_mape,\n",
    "    best_overall_mape_score=best_overall_mape_score,\n",
    "    test_error_mape=test_error_mape,\n",
    "    test_error_mse_reg=test_error_mse_reg, \n",
    "    result_final_train_message = result_final_train_message,\n",
    "    final_train_mse_no_reg= final_train_mse_no_reg , test_error_mse_no_reg = test_mse_no_reg, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
