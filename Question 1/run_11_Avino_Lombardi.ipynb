{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ed43a0",
   "metadata": {},
   "source": [
    "# Part 1 â€” Multi-Layer Perceptron (Age Regression)\n",
    "\n",
    "| Name            | Student ID | Email                                      |\n",
    "|-----------------|------------|--------------------------------------------|\n",
    "| Valeria Avino   | 1905974    | avino.1905974@studenti.uniroma1.it         |\n",
    "| Marta Lombardi  | 2156537    | lombardi.2156537@studenti.uniroma1.it      |\n",
    "\n",
    "\n",
    "This notebook presents the implementation of a custom Multi-Layer Perceptron (MLP) for a **regression task** using the dataset `AGE REGRESSION.csv`. The goal is to minimize the **L2 regularized loss function** using manually derived gradients and a numerical optimizer provided by `scipy.optimize`.\n",
    "\n",
    "The neural network:\n",
    "- Uses **at least two hidden layers**\n",
    "- Applies **L2 regularization** to the weight matrices\n",
    "- Supports **sigmoid** or **tanh** activation functions\n",
    "- Is optimized via **L-BFGS-B**, without relying on automatic differentiation libraries\n",
    "- Evaluates performance with the **Mean Absolute Percentage Error (MAPE)**\n",
    "- Selects hyperparameters using **k-fold cross-validation**\n",
    "\n",
    "This notebook is structured to:\n",
    "1. Preprocess the dataset\n",
    "2. Define and optimize the MLP model from scratch\n",
    "3. Perform model selection using cross-validation\n",
    "4. Report training, validation, and test results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4828f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "import time\n",
    "\n",
    "# used in the py file, here for explanability\n",
    "from typing import Callable, Tuple, List, Dict, Any\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "\n",
    "# importing auxiliary functions from Functions.py file\n",
    "from Functions_11_Avino_Lombardi import (\n",
    "    forward, backward,\n",
    "    g1, dg1_dx, g2, dg2_dx,\n",
    "    mse_loss, mape, xavier_normal_init,\n",
    "    initialize_parameters, unroll_params, roll_params,\n",
    "    check_gradients_with_central_differences,\n",
    "    objective_function, final_report_metrics, myMLPRegressor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5365d39",
   "metadata": {},
   "source": [
    "### Data loading and splitting\n",
    "After loading the data in our environment, we randomly partition it into training ( $80\\% $) and testing ( $20\\%$ ) sets. This separation is fundamental to evaluate the generalization capability of our trained model on unseen (test) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af05b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape: (20475, 32)\n",
      "Initial y shape: (20475, 1)\n",
      "D_input (number of input features): 32\n",
      "y_output_dim (number of output dimensions): 1\n",
      "\n",
      "X_train shape after split: (32, 16380)\n",
      "y_train shape after split: (1, 16380)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/Val/Documents/GitHub/OMDS-Project/dataset/AGE_PREDICTION.csv')\n",
    "\n",
    "# Separate features (X_full) and target (y_full) from the entire dataset\n",
    "feature_columns = [f'feat_{i}' for i in range(1, 33)]\n",
    "X_full = df[feature_columns].values # Shape will be [N, D]\n",
    "y_full = df['gt'].values.reshape(-1, 1) # Shape will be [N, 1]\n",
    "\n",
    "\n",
    "print(f\"Initial X shape: {X_full.shape}\")\n",
    "print(f\"Initial y shape: {y_full.shape}\")\n",
    "\n",
    "# Get total number of samples\n",
    "N_samples_full = X_full.shape[0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=1234)\n",
    "\n",
    "X_train = X_train.T # Transpose to [D, Ntrain]\n",
    "y_train = y_train.T # Transpose to [1, Ntrain]\n",
    "X_test = X_test.T # Transpose to [D, N-Ntrain]\n",
    "y_test = y_test.T # Transpose to [1, N-Ntrain]\n",
    "\n",
    "# Determine D_input (number of input features) - from training data to check shapes\n",
    "D_input = X_train.shape[0]\n",
    "y_output_dim = y_train.shape[0]\n",
    "print(f\"D_input (number of input features): {D_input}\")\n",
    "print(f\"y_output_dim (number of output dimensions): {y_output_dim}\\n\")\n",
    "print(f\"X_train shape after split: {X_train.shape}\") # (N_features, N_train)\n",
    "print(f\"y_train shape after split: {y_train.shape}\") # (1, N_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d1cb2",
   "metadata": {},
   "source": [
    "### Feature standardization\n",
    "\n",
    "Before applying any optimization routine, we normalize our data\n",
    "For each feature $x_i$:\n",
    "$$x_i^{\\text{normalized}}=\\frac{x_i-\\mu_i}{\\sigma_i}$$\n",
    "\n",
    "where $\\mu_i$ and $\\sigma_i$ are the **mean** and **standard deviation** of the $i^{th}$ feature, computed over the **training set**, since at this stage we do not have access to test information. The same transformation is then applied to test data.\n",
    "\n",
    "Standardization ensures __all features contribute equally__ to the loss landscape, thus to the gradient updates, prevents issues like vanishing or exploding gradients due to varying feature scales, and accelerates the convergence of our L-BFGS-B optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3172e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual, but can be done with \"StandardScaler\"\n",
    "# only computed from train data\n",
    "mu = X_train.mean(axis=1, keepdims=True)\n",
    "sigma = X_train.std(axis=1, keepdims=True)\n",
    "\n",
    "# Handle cases where standard deviation might be zero (constant feature)\n",
    "sigma[sigma == 0] = 1e-8 \n",
    "\n",
    "# Apply the transformation to the TRAINING DATA\n",
    "X_train_normalized = (X_train - mu) / sigma\n",
    "\n",
    "# Apply the *SAME* transformation (using mu and sigma from training) to the TEST DATA\n",
    "X_test_normalized = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88984742",
   "metadata": {},
   "source": [
    "Let's check if the gradient was correctly computed by the __Central differences__ approximation:\n",
    "$$f'(x) \\approx \\frac{f(x+\\Delta x)-f(x-\\Delta x)}{2\\Delta x}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4344d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing gradient check...\n",
      "\n",
      "--- Gradient Check (Central Differences) ---\n",
      "Checking 200 parameters...\n",
      "Analytical loss at initial point: 1799.840240\n",
      "\n",
      "Gradient check PASSED! Norm of difference: 1.566936e-06\n",
      "------------------------------------------\n",
      "Gradient check finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming gradient check...\")\n",
    "\n",
    "# subset of training data for gradient check \n",
    "num_samples_for_check = min(1000, X_train_normalized.shape[1])\n",
    "X_check_subset = X_train_normalized[:, :num_samples_for_check]\n",
    "y_check_subset = y_train[:, :num_samples_for_check]\n",
    "\n",
    "# toy set of hyperparameters for the check\n",
    "check_L = 3\n",
    "check_neurons_config = [5, 5]\n",
    "check_activation_func = g2 # also checked with g1 and different reg factors\n",
    "check_activation_prime = dg2_dx\n",
    "check_reg_factor = 0.001\n",
    "\n",
    "# initialize parameters\n",
    "np.random.seed( 1234 )\n",
    "W_check_init, b_check_init, v_check_init = initialize_parameters(\n",
    "    D_input, check_neurons_config, y_output_dim, check_reg_factor\n",
    ")\n",
    "initial_flat_params_for_check = unroll_params(W_check_init, b_check_init, v_check_init)\n",
    "\n",
    "W_shapes_for_check = [W.shape for W in W_check_init]\n",
    "b_shapes_for_check = [b.shape for b in b_check_init]\n",
    "v_shape_for_check = v_check_init.shape\n",
    "\n",
    "# gradient check function\n",
    "check_gradients_with_central_differences(\n",
    "    initial_flat_params_for_check,\n",
    "    X_check_subset, y_check_subset,\n",
    "    W_shapes_for_check, b_shapes_for_check, v_shape_for_check,\n",
    "    check_activation_func, check_activation_prime,\n",
    "    check_reg_factor, check_L,\n",
    "    objective_function\n",
    ")\n",
    "\n",
    "print(\"Gradient check finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e5c37",
   "metadata": {},
   "source": [
    "## Custom MLP Regressor Class\n",
    "\n",
    "In `Functions_11_Avino_lombardi.py` we define a custom multi-layer perceptron (MLP) regressor class that integrates with the `scikit-learn` API by subclassing `BaseEstimator` (for hyperparameter handling and GridSearchCV support) and `RegressorMixin` (to behave like a standard sklearn regressor). It allows easy use of cross-validation and hyperparameter tuning via `GridSearchCV`.\n",
    "\n",
    "The class implements:\n",
    "\n",
    "- **Manual forward and backward propagation** (no autograd),\n",
    "- **L2-regularized loss function** (objective),\n",
    "- **Training via `scipy.optimize.minimize` with L-BFGS-B**, using analytical gradients,\n",
    "- **Support for different activation functions** (tanh or sigmoid),\n",
    "- **Hyperparameters** such as number of layers, neurons per layer and regularization strength.\n",
    "\n",
    "It defines `.fit()` and `.predict()` methods following the sklearn standard, and it prints training loss periodically when enabled. This class is the core object used in the training and evaluation workflow that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46184f20",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation + Grid Search\n",
    " \n",
    "To identify the optimal MLP architecture and hyperparameters, we perform **GridSearchCV** using **5-fold cross-validation**. \n",
    "\n",
    "#### Cross-Validation Strategy\n",
    "\n",
    "We use `KFold` with `k = 5`, meaning the training data is split into 5 disjoint subsets. For each fold:\n",
    "- The model is trained on 4 folds.\n",
    "- It is validated on the remaining 1 fold.\n",
    "\n",
    "This process repeats 5 times so that every fold serves once as validation.\n",
    "\n",
    "#### Evaluation Metric: MAPE\n",
    "\n",
    "We define a **custom MAPE scorer** (Mean Absolute Percentage Error) using `mape` function form our py file and `make_scorer` from `sklearn.metrics`, and negate it so that `GridSearchCV` treats **lower MAPE as better** (since it maximizes the score).\n",
    "\n",
    "#### Grid Search Space\n",
    "\n",
    "We define a grid of hyperparameter combinations over:\n",
    "- **Total number of layers** $L \\in \\{3, 4, 5\\}$ (implying 2â€“4 hidden layers)\n",
    "- **Hidden layer sizes**, e.g., `[32, 16]`, `[32, 16, 32]`\n",
    "- **Activation function**: `'g1'` (tanh) or `'g2'` (sigmoid)\n",
    "- **L2 regularization factor**: $\\lambda \\in \\{0.001, 0.01\\}$\n",
    "\n",
    "Only valid combinations are retained (i.e., where the number of hidden layers matches the length of the neuron configuration).\n",
    "\n",
    "#### Execution\n",
    "\n",
    "We pass our custom `myMLPRegressor` estimator to `GridSearchCV`, along with:\n",
    "- the filtered hyperparameter grid,\n",
    "- the `KFold` splitter,\n",
    "- the custom `MAPE` scorer,\n",
    "- `n_jobs=-1` for parallelism.\n",
    "\n",
    "The grid search returns the combination with the **lowest average validation MAPE** across all 5 folds, which is then used to retrain the final model on the full training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a847060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sklearn shape (for sklearn): (16380, 32)\n",
      "y_train_sklearn shape (for sklearn): (16380, 1)\n",
      "\n",
      "Starting GridSearchCV based Hyperparameter Tuning...\n",
      "\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "\n",
      "GridSearchCV completed.\n",
      "GridSearchCV took 1510.16 seconds to complete.\n"
     ]
    }
   ],
   "source": [
    "# Custom MAPE Scorer for GridSearchCV\n",
    "# best = highest score --> negative mape \n",
    "mape_scorer = make_scorer(lambda y_true, y_pred: -mape(y_true.reshape(1,-1), y_pred.reshape(1,-1)), greater_is_better=True)\n",
    "\n",
    "# X_train_normalized and y_train are now (D, N) and (1, N) \n",
    "# For scikit-learn, they need to be (N, D) and (N, 1).\n",
    "X_train_sklearn = X_train_normalized.T\n",
    "y_train_sklearn = y_train.T\n",
    "\n",
    "X_test_sklearn = X_test_normalized.T\n",
    "y_test_sklearn = y_test.T \n",
    "\n",
    "print(f\"X_train_sklearn shape (for sklearn): {X_train_sklearn.shape}\")\n",
    "print(f\"y_train_sklearn shape (for sklearn): {y_train_sklearn.shape}\\n\")\n",
    "\n",
    "\n",
    "# Hyperparameter Search Space for GridSearchCV\n",
    "param_grid_full = {\n",
    "    'num_layers': [3, 4, 5], # Total no. of layers\n",
    "    'num_neurons': [\n",
    "        [32, 16], [32, 32], [64, 32],\n",
    "        [16, 16, 16], [32, 16, 32], [32, 32, 32],\n",
    "        [16, 8, 16, 8], [32, 16, 32, 16] \n",
    "    ],\n",
    "    'activation_func_name': ['g1', 'g2'], \n",
    "    'regularization_factor': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Keep only VALID combinations\n",
    "filtered_param_grid = []\n",
    "for L_val in param_grid_full['num_layers']:\n",
    "    # Number of hidden layers is L_val - 1 (L is total layers, including output)\n",
    "    expected_hidden_layers = L_val - 1\n",
    "    for N_list_val in param_grid_full['num_neurons']:\n",
    "        if len(N_list_val) == expected_hidden_layers:\n",
    "            for act_name_val in param_grid_full['activation_func_name']:\n",
    "                for reg_f_val in param_grid_full['regularization_factor']:\n",
    "                    filtered_param_grid.append({\n",
    "                        'num_layers': [L_val],\n",
    "                        'num_neurons': [N_list_val],\n",
    "                        'activation_func_name': [act_name_val],\n",
    "                        'regularization_factor': [reg_f_val],\n",
    "                    })\n",
    "\n",
    "# Instantiate the MLP Regressor for cv, with a reduced maxiter for speed\n",
    "mlp_estimator = myMLPRegressor(D_input=D_input, y_output_dim=y_output_dim, max_iter=500, print_callback_loss=False, random_state=1234) # no loss prints here\n",
    "\n",
    "# Setup KFold for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "print(\"Starting GridSearchCV based Hyperparameter Tuning...\\n\")\n",
    "\n",
    "# Set up GridSearchCV \n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp_estimator,\n",
    "    param_grid=filtered_param_grid, \n",
    "    cv=kf,\n",
    "    scoring=mape_scorer, \n",
    "    verbose=2, \n",
    "    n_jobs=-1,\n",
    "    error_score=np.nan ,\n",
    "    refit=True # for initial loss calculation\n",
    ")\n",
    "\n",
    "# Run the grid search \n",
    "start_time_grid_search = time.time()\n",
    "grid_search.fit(X_train_sklearn, y_train_sklearn)\n",
    "end_time_grid_search = time.time()\n",
    "grid_search_time = end_time_grid_search - start_time_grid_search\n",
    "\n",
    "print(\"\\nGridSearchCV completed.\")\n",
    "print(f\"GridSearchCV took {grid_search_time:.2f} seconds to complete.\")\n",
    "\n",
    "# Results = Best hyperparameters\n",
    "best_overall_params = grid_search.best_params_\n",
    "best_overall_mape_score = -grid_search.best_score_ # return to mape (positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e60f5",
   "metadata": {},
   "source": [
    "### Retraining the optimal model and Testing\n",
    "\n",
    "After identifying the best-performing hyperparameter combination via cross-validation, we now **retrain the model** using these optimal settings on the **entire training dataset**. This allows the model to fully leverage all available training data for learning.\n",
    "\n",
    "Once training is complete, we evaluate the modelâ€™s **generalization performance** by predicting on the **held-out test set**, which has remained completely unseen throughout training and validation. This final evaluation provides a realistic estimate of the modelâ€™s predictive accuracy on new, real-world data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7034af35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retraining Optimal Model on Full TRAINING Dataset (max_iter=5000) ---\n",
      "\n",
      "Final training uses Optimal Configuration:\n",
      "  Layers (L): 4\n",
      "  Neurons per Layer: [16, 16, 16]\n",
      "  Activation Function: g2\n",
      "  Regularization Factor: 0.001\n",
      "  Max Iterations for L-BFGS-B (Final Train): 5000\n",
      "  Iteration 10: Non-regularized MSE Loss = 140.060655\n",
      "  Iteration 20: Non-regularized MSE Loss = 97.209427\n",
      "  Iteration 30: Non-regularized MSE Loss = 96.100711\n",
      "  Iteration 40: Non-regularized MSE Loss = 95.665333\n",
      "  Iteration 50: Non-regularized MSE Loss = 95.495086\n",
      "  Iteration 60: Non-regularized MSE Loss = 95.175409\n",
      "  Iteration 70: Non-regularized MSE Loss = 95.008286\n",
      "  Iteration 80: Non-regularized MSE Loss = 94.797841\n",
      "  Iteration 90: Non-regularized MSE Loss = 94.583161\n",
      "  Iteration 100: Non-regularized MSE Loss = 94.505955\n",
      "  Iteration 110: Non-regularized MSE Loss = 94.377219\n",
      "  Iteration 120: Non-regularized MSE Loss = 94.269977\n",
      "  Iteration 130: Non-regularized MSE Loss = 94.147587\n",
      "  Iteration 140: Non-regularized MSE Loss = 94.057947\n",
      "  Iteration 150: Non-regularized MSE Loss = 93.966873\n",
      "  Iteration 160: Non-regularized MSE Loss = 93.898684\n",
      "  Iteration 170: Non-regularized MSE Loss = 93.838147\n",
      "  Iteration 180: Non-regularized MSE Loss = 93.778391\n",
      "  Iteration 190: Non-regularized MSE Loss = 93.711761\n",
      "  Iteration 200: Non-regularized MSE Loss = 93.663050\n",
      "  Iteration 210: Non-regularized MSE Loss = 93.606957\n",
      "  Iteration 220: Non-regularized MSE Loss = 93.551902\n",
      "  Iteration 230: Non-regularized MSE Loss = 93.509534\n",
      "  Iteration 240: Non-regularized MSE Loss = 93.460380\n",
      "  Iteration 250: Non-regularized MSE Loss = 93.412190\n",
      "  Iteration 260: Non-regularized MSE Loss = 93.360789\n",
      "  Iteration 270: Non-regularized MSE Loss = 93.318103\n",
      "  Iteration 280: Non-regularized MSE Loss = 93.277459\n",
      "  Iteration 290: Non-regularized MSE Loss = 93.249999\n",
      "  Iteration 300: Non-regularized MSE Loss = 93.219488\n",
      "  Iteration 310: Non-regularized MSE Loss = 93.195123\n",
      "  Iteration 320: Non-regularized MSE Loss = 93.143077\n",
      "  Iteration 330: Non-regularized MSE Loss = 93.112346\n",
      "\n",
      "Final model training completed in 18.29 seconds over 337 iterations.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Retraining Optimal Model on Full TRAINING Dataset (max_iter=5000) ---\")\n",
    "\n",
    "# Extract final parameters from best_overall_params\n",
    "final_L = best_overall_params['num_layers']\n",
    "final_neurons_config = best_overall_params['num_neurons']\n",
    "final_activation_name = best_overall_params['activation_func_name']\n",
    "final_reg_factor = best_overall_params['regularization_factor']\n",
    "final_train_max_iter = 5000\n",
    "\n",
    "final_activation_func = g1 if final_activation_name == 'g1' else g2\n",
    "final_activation_prime = dg1_dx if final_activation_name == 'g1' else dg2_dx\n",
    "\n",
    "print(f\"\\nFinal training uses Optimal Configuration:\")\n",
    "print(f\"  Layers (L): {final_L}\")\n",
    "print(f\"  Neurons per Layer: {final_neurons_config}\")\n",
    "print(f\"  Activation Function: {final_activation_func.__name__}\")\n",
    "print(f\"  Regularization Factor: {final_reg_factor}\")\n",
    "print(f\"  Max Iterations for L-BFGS-B (Final Train): {final_train_max_iter}\")\n",
    "\n",
    "\n",
    "final_model = myMLPRegressor(\n",
    "    D_input=D_input,\n",
    "    y_output_dim=y_output_dim, \n",
    "    num_layers=final_L,\n",
    "    num_neurons=final_neurons_config,\n",
    "    activation_func_name=final_activation_name,\n",
    "    regularization_factor=final_reg_factor,\n",
    "    max_iter=final_train_max_iter,\n",
    "    print_callback_loss=True, # callback printing to track mse loss behaviour\n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "# Calculate Initial Training Error (Regularized MSE & MAPE) \n",
    "\n",
    "np.random.seed(1234)\n",
    "W_final_init, b_final_init, v_final_init = initialize_parameters(\n",
    "    D_input, final_neurons_config, y_output_dim, final_reg_factor\n",
    ")\n",
    "initial_flat_params_final_model = unroll_params(W_final_init, b_final_init, v_final_init)\n",
    "\n",
    "W_shapes_final_model = [W.shape for W in W_final_init]\n",
    "b_shapes_final_model = [b.shape for b in b_final_init] \n",
    "v_shape_final_model = v_final_init.shape\n",
    "\n",
    "y_train_pred_initial_final_model, _, _ = forward(\n",
    "    X_train_normalized, W_final_init, b_final_init, v_final_init, final_activation_func, final_L\n",
    ")\n",
    "initial_train_mape_final_model = mape(y_train, y_train_pred_initial_final_model)\n",
    "initial_train_mse_reg_final_model, _ = objective_function(\n",
    "    initial_flat_params_final_model,\n",
    "    X_train_normalized, y_train, W_shapes_final_model, b_shapes_final_model, v_shape_final_model,\n",
    "    final_activation_func, final_activation_prime, final_reg_factor, final_L\n",
    ")\n",
    "\n",
    "# Final Training on Full Training Data by calling .fit() on myMLPRegressor instance\n",
    "start_time_final_train = time.time()\n",
    "final_model.fit(X_train_sklearn, y_train_sklearn) \n",
    "end_time_final_train = time.time()\n",
    "\n",
    "# predict on train to get train MAPE\n",
    "y_train_pred_final = final_model.predict(X_train_sklearn)\n",
    "final_train_mape = mape(y_train, y_train_pred_final)\n",
    "final_train_mse_no_reg = mse_loss(y_train, y_train_pred_final)\n",
    "\n",
    "# Extract results directly from final_model's attributes\n",
    "final_training_time = end_time_final_train - start_time_final_train\n",
    "final_training_iterations = final_model.n_iterations_\n",
    "final_train_objective_value = final_model.final_objective_value_\n",
    "result_final_train_message = final_model.optimization_message_ # message from the fitted model\n",
    "\n",
    "print(f\"\\nFinal model training completed in {final_training_time:.2f} seconds over {final_training_iterations} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9536e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test Error (MAPE): 20.0649%\n",
      "  Test Error (MSE, regularized): 95.7957\n",
      "  Test Error (MSE, non-regularized): 94.7284\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_raw = final_model.predict(X_test_sklearn)\n",
    "test_error_mape = mape(y_test, y_test_pred_raw.reshape(1,-1))\n",
    "final_trained_flat_params = unroll_params(final_model.W_list_, final_model.b_list_, final_model.v_)\n",
    "test_mse_no_reg = mse_loss(y_test, y_test_pred_raw)\n",
    "\n",
    "\n",
    "test_error_mse_reg, _ = objective_function(\n",
    "    final_trained_flat_params,\n",
    "    X_test_normalized, y_test, # Use X_test_normalized and y_test (original D,N and 1,N)\n",
    "    final_model.W_shapes_, final_model.b_shapes_, final_model.v_shape_,\n",
    "    final_model.activation_func_, final_model.activation_prime_, final_model.regularization_factor, final_model.num_layers\n",
    ")\n",
    "print(f\"  Test Error (MAPE): {test_error_mape:.4f}%\")\n",
    "print(f\"  Test Error (MSE, regularized): {test_error_mse_reg:.4f}\")\n",
    "print(f\"  Test Error (MSE, non-regularized): {test_mse_no_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85f4378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Functions_11_Avino_Lombardi\n",
    "importlib.reload(Functions_11_Avino_Lombardi)\n",
    "from Functions_11_Avino_Lombardi import final_report_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fefa1",
   "metadata": {},
   "source": [
    "### Final Report Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ef9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comprehensive Performance Metrics for Final Report ---\n",
      "\n",
      "1. Optimal Model Configuration:\n",
      "  Non-linearity (Activation Function): g2\n",
      "  Total Number of Layers (L): 4\n",
      "  Neurons per Layer (Nl): [16, 16, 16]\n",
      "  Regularization Factor (Î»): 0.001\n",
      "\n",
      "2. Optimization Routine Details (for Final Training):\n",
      "  Optimization Routine: L-BFGS-B\n",
      "  Max Number of Iterations Parameter: 5000\n",
      "  Returned Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  Number of Iterations Performed: 337\n",
      "  Starting Value of Objective Function: 1.7427e+03\n",
      "  Final Value of Objective Function: 9.4165e+01\n",
      "\n",
      "3. Training Set Performance:\n",
      "  Initial Training Error (MAPE): 3569.5107%\n",
      "  Initial Training Error (MSE, regularized): 1742.7159\n",
      "  Final Training Error (MAPE): 20.1776%\n",
      "  Final Training Error (MSE, regularized): 94.1650\n",
      "  Final Training Error (MSE, **non-regularized**): 93.0976\n",
      "\n",
      "4. Validation Set Performance (Average from K-Fold CV):\n",
      "  Average Validation Error (MAPE): 20.3638%\n",
      "\n",
      "5. Test Set Performance:\n",
      "  Final Test Error (MAPE): 20.0649%\n",
      "  Final Test Error (MSE, regularized): 95.7957\n",
      "  Final Test Error (MSE, **non-regularized**): 94.7284\n",
      "\n",
      "Performance Metrics Summary Table for Report (Figure 1 Data):\n",
      "Metric                    Training (Final)     Test (Final)         Validation (Avg)    \n",
      "-------------------------------------------------------------------------------------\n",
      "MAPE (%)                  20.1776%                  20.0649%                 20.3638%                  \n",
      "Regularized MSE           94.1650                   95.7957                  \n",
      "MSE (no reg)              93.0976                   94.7284                  \n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# auxiliary function to display metrics for report\n",
    "final_report_metrics(\n",
    "    final_L=final_L,\n",
    "    final_neurons_config=final_neurons_config,\n",
    "    final_activation_name=final_activation_name,\n",
    "    final_reg_factor=final_reg_factor,\n",
    "    final_train_max_iter=final_train_max_iter,\n",
    "    final_training_iterations=final_training_iterations,\n",
    "    initial_train_mse_reg_final_model=initial_train_mse_reg_final_model,\n",
    "    final_train_objective_value=final_train_objective_value,\n",
    "    initial_train_mape_final_model=initial_train_mape_final_model,\n",
    "    final_train_mape=final_train_mape,\n",
    "    best_overall_mape_score=best_overall_mape_score,\n",
    "    test_error_mape=test_error_mape,\n",
    "    test_error_mse_reg=test_error_mse_reg, \n",
    "    result_final_train_message = result_final_train_message,\n",
    "    final_train_mse_no_reg= final_train_mse_no_reg , test_error_mse_no_reg = test_mse_no_reg, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
